"""ç¶²é çˆ¬èŸ²æœå‹™æ¨¡çµ„ã€‚

æ­¤æ¨¡çµ„æä¾›ç¶²é å…§å®¹çˆ¬å–åŠŸèƒ½ï¼ŒåŒ…æ‹¬ä¸¦è¡Œçˆ¬å–ã€HTML è§£æã€
SEO å…ƒç´ æå–ã€éŒ¯èª¤è™•ç†å’Œé‡è©¦æ©Ÿåˆ¶ã€‚
"""

import asyncio
import time
from dataclasses import dataclass
from typing import List, Dict, Any, Optional

import aiohttp
from aiohttp import ClientError
from bs4 import BeautifulSoup, Tag
from bs4.element import NavigableString

from ..config import get_config


# è‡ªå®šç¾©ä¾‹å¤–é¡åˆ¥
class ScraperException(Exception):
    """çˆ¬èŸ²ç›¸é—œéŒ¯èª¤çš„åŸºç¤ä¾‹å¤–é¡åˆ¥ã€‚"""


class ScraperTimeoutException(ScraperException):
    """çˆ¬èŸ²é€¾æ™‚éŒ¯èª¤ï¼Œå°æ‡‰ API éŒ¯èª¤ç¢¼ SCRAPER_TIMEOUTã€‚"""


class ScraperParsingException(ScraperException):
    """HTML è§£æéŒ¯èª¤ã€‚"""


# è³‡æ–™çµæ§‹å®šç¾©
@dataclass
class PageContent:
    """çˆ¬å–çš„ç¶²é å…§å®¹è³‡æ–™çµæ§‹ã€‚
    
    Attributes:
        url: åŸå§‹ URL
        title: é é¢æ¨™é¡Œ (<title> æ¨™ç±¤)
        meta_description: Meta æè¿°æ¨™ç±¤å…§å®¹
        h1: ä¸»æ¨™é¡Œå…§å®¹
        h2_list: å‰¯æ¨™é¡Œæ¸…å–®
        word_count: å…§æ–‡å­—æ•¸çµ±è¨ˆ
        paragraph_count: æ®µè½æ•¸çµ±è¨ˆ
        status_code: HTTP å›æ‡‰ç‹€æ…‹ç¢¼
        load_time: é é¢è¼‰å…¥æ™‚é–“ (ç§’)
        success: æ˜¯å¦æˆåŠŸçˆ¬å–
        error: éŒ¯èª¤è¨Šæ¯ (å¦‚æœæœ‰)
    """
    url: str
    h2_list: List[str]
    title: Optional[str] = None
    meta_description: Optional[str] = None
    h1: Optional[str] = None
    word_count: int = 0
    paragraph_count: int = 0
    status_code: int = 0
    load_time: float = 0.0
    success: bool = False
    error: Optional[str] = None


@dataclass
class ScrapingResult:
    """æ‰¹é‡çˆ¬å–çµæœè³‡æ–™çµæ§‹ã€‚
    
    ç¬¦åˆ API è¦æ ¼ä¸­ serp_summary çš„æ ¼å¼è¦æ±‚ã€‚
    
    Attributes:
        total_results: ç¸½ URL æ•¸é‡
        successful_scrapes: æˆåŠŸçˆ¬å–æ•¸é‡
        avg_word_count: å¹³å‡å­—æ•¸
        avg_paragraphs: å¹³å‡æ®µè½æ•¸
        pages: å„é é¢è©³ç´°å…§å®¹
        errors: éŒ¯èª¤è³‡è¨Šæ¸…å–®
    """
    total_results: int
    successful_scrapes: int
    avg_word_count: int
    avg_paragraphs: int
    pages: List[PageContent]
    errors: List[Dict[str, Any]]


class ScraperService:
    """ç¶²é çˆ¬èŸ²æœå‹™é¡åˆ¥ã€‚
    
    æä¾›ä¸¦è¡Œç¶²é çˆ¬å–ã€HTML è§£æã€SEO å…ƒç´ æå–ç­‰åŠŸèƒ½ï¼Œ
    æ”¯æ´é‡è©¦æ©Ÿåˆ¶ã€é€¾æ™‚æ§åˆ¶å’ŒéŒ¯èª¤è™•ç†ã€‚
    """

    def __init__(self):
        """åˆå§‹åŒ–çˆ¬èŸ²æœå‹™ã€‚
        
        è¼‰å…¥é…ç½®ä¸¦è¨­å®šçˆ¬èŸ²åƒæ•¸ã€‚
        """
        self.config = get_config()
        
        # çˆ¬èŸ²é…ç½®åƒæ•¸
        self.max_concurrent = self.config.get_scraper_max_concurrent()
        self.timeout = self.config.get_scraper_timeout()
        self.max_retries = self.config.get_scraper_retry_count()
        self.retry_delay = self.config.get_scraper_retry_delay()
        
        # HTTP è«‹æ±‚é…ç½®
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36'
        ]
        
    async def scrape_urls(self, urls: List[str]) -> ScrapingResult:
        """æ‰¹é‡çˆ¬å– URL æ¸…å–®ã€‚
        
        ä½¿ç”¨ä¸¦è¡Œè™•ç†çˆ¬å–å¤šå€‹ URLï¼Œæä¾›å®Œæ•´çš„çµ±è¨ˆè³‡è¨Šã€‚
        
        Args:
            urls: è¦çˆ¬å–çš„ URL æ¸…å–®
            
        Returns:
            ScrapingResult: åŒ…å«çµ±è¨ˆè³‡è¨Šå’Œå„é é¢å…§å®¹çš„çµæœ
            
        Raises:
            ScraperException: çˆ¬èŸ²åŸ·è¡Œéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤
        """
        if not urls:
            return ScrapingResult(
                total_results=0,
                successful_scrapes=0,
                avg_word_count=0,
                avg_paragraphs=0,
                pages=[],
                errors=[]
            )
        
        start_time = time.time()
        
        # ä½¿ç”¨ Semaphore æ§åˆ¶ä¸¦è¡Œæ•¸é‡
        semaphore = asyncio.Semaphore(self.max_concurrent)
        
        # å»ºç«‹ä¸¦è¡Œä»»å‹™
        tasks = [
            self._scrape_single_url_with_semaphore(semaphore, url)
            for url in urls
        ]
        
        # åŸ·è¡Œä¸¦è¡Œçˆ¬å–
        try:
            pages = await asyncio.gather(*tasks, return_exceptions=True)
        except Exception as e:
            raise ScraperException(f"ä¸¦è¡Œçˆ¬å–åŸ·è¡Œå¤±æ•—: {str(e)}")
        
        # è™•ç†çµæœå’Œä¾‹å¤–
        successful_pages = []
        errors = []
        
        for i, result in enumerate(pages):
            if isinstance(result, Exception):
                errors.append({
                    'url': urls[i],
                    'error': str(result),
                    'error_type': type(result).__name__
                })
            elif isinstance(result, PageContent):
                if result.success:
                    successful_pages.append(result)
                else:
                    errors.append({
                        'url': result.url,
                        'error': result.error or 'Unknown error',
                        'error_type': 'ScrapingError'
                    })
        
        # è¨ˆç®—çµ±è¨ˆè³‡è¨Š
        total_results = len(urls)
        successful_scrapes = len(successful_pages)
        
        if successful_pages:
            avg_word_count = int(sum(page.word_count for page in successful_pages) / len(successful_pages))
            avg_paragraphs = int(sum(page.paragraph_count for page in successful_pages) / len(successful_pages))
        else:
            avg_word_count = 0
            avg_paragraphs = 0
        
        processing_time = time.time() - start_time
        
        # æª¢æŸ¥æ˜¯å¦é”åˆ°æœ€ä½æˆåŠŸç‡è¦æ±‚ (80%)
        success_rate = successful_scrapes / total_results if total_results > 0 else 0
        if success_rate < 0.8:
            # è¨˜éŒ„è­¦å‘Šä½†ä¸æ‹‹å‡ºä¾‹å¤–ï¼Œå…è¨±éƒ¨åˆ†å¤±æ•—
            print(f"è­¦å‘Šï¼šçˆ¬èŸ²æˆåŠŸç‡ {success_rate:.1%} ä½æ–¼ 80% ç›®æ¨™")
        
        print(f"çˆ¬èŸ²å®Œæˆï¼š{successful_scrapes}/{total_results} æˆåŠŸï¼Œè€—æ™‚ {processing_time:.2f} ç§’")
        
        # å°å‡ºæ¯ç­†URLè³‡æ–™çš„å‰100å­—å…ƒ
        print("ğŸ“„ ç¶²é çˆ¬å–å…§å®¹é è¦½ï¼š")
        for i, page in enumerate([p for p in pages if isinstance(p, PageContent)], 1):
            if page.success:
                # çµ„åˆå¯ç”¨å…§å®¹ä½œç‚ºé è¦½ï¼ˆPageContent æ²’æœ‰ content å±¬æ€§ï¼Œä½¿ç”¨ title, h1, meta_descriptionï¼‰
                content_parts = []
                if page.title:
                    content_parts.append(f"æ¨™é¡Œ: {page.title}")
                if page.h1:
                    content_parts.append(f"H1: {page.h1}")
                if page.meta_description:
                    content_parts.append(f"æè¿°: {page.meta_description}")
                
                content_preview = " | ".join(content_parts)[:100]
                print(f"  {i}. URL: {page.url}")
                print(f"     å…§å®¹: {content_preview}{'...' if len(content_preview) > 100 else ''}")
                print(f"     å­—æ•¸: {page.word_count}, æ®µè½: {page.paragraph_count}")
                print(f"     H2æ¨™ç±¤: {len(page.h2_list)} å€‹")
            else:
                print(f"  {i}. URL: {page.url}")
                print(f"     ç‹€æ…‹: çˆ¬å–å¤±æ•— - {page.error if page.error else 'æœªçŸ¥éŒ¯èª¤'}")
            print()
        
        return ScrapingResult(
            total_results=total_results,
            successful_scrapes=successful_scrapes,
            avg_word_count=avg_word_count,
            avg_paragraphs=avg_paragraphs,
            pages=successful_pages + [page for page in pages if isinstance(page, PageContent) and not page.success],
            errors=errors
        )
    
    async def _scrape_single_url_with_semaphore(self, semaphore: asyncio.Semaphore, url: str) -> PageContent:
        """ä½¿ç”¨ Semaphore æ§åˆ¶çš„å–®é çˆ¬å–ã€‚
        
        Args:
            semaphore: ç”¨æ–¼æ§åˆ¶ä¸¦è¡Œæ•¸é‡çš„ Semaphore
            url: è¦çˆ¬å–çš„ URL
            
        Returns:
            PageContent: çˆ¬å–çµæœ
        """
        async with semaphore:
            return await self.scrape_single_url(url)
    
    async def scrape_single_url(self, url: str) -> PageContent:
        """çˆ¬å–å–®å€‹ URL çš„å…§å®¹ã€‚
        
        åŒ…å«é‡è©¦æ©Ÿåˆ¶å’Œå®Œæ•´çš„éŒ¯èª¤è™•ç†ã€‚
        
        Args:
            url: è¦çˆ¬å–çš„ URL
            
        Returns:
            PageContent: çˆ¬å–çš„é é¢å…§å®¹
        """
        start_time = time.time()
        last_error = None
        
        for attempt in range(self.max_retries):
            try:
                return await self._execute_scraping(url, start_time)
            except asyncio.TimeoutError:
                last_error = ScraperTimeoutException(f"URL {url} çˆ¬å–é€¾æ™‚")
                if attempt < self.max_retries - 1:
                    await asyncio.sleep(self.retry_delay * (2 ** attempt))
                    continue
            except ClientError as e:
                last_error = ScraperException(f"ç¶²è·¯éŒ¯èª¤: {str(e)}")
                if attempt < self.max_retries - 1:
                    await asyncio.sleep(self.retry_delay * (2 ** attempt))
                    continue
            except Exception as e:
                last_error = ScraperException(f"æœªé æœŸéŒ¯èª¤: {str(e)}")
                break  # éç¶²è·¯éŒ¯èª¤ä¸é‡è©¦
        
        # æ‰€æœ‰é‡è©¦éƒ½å¤±æ•—ï¼Œå›å‚³å¤±æ•—çµæœ
        load_time = time.time() - start_time
        return PageContent(
            url=url,
            h2_list=[],
            load_time=load_time,
            success=False,
            error=str(last_error) if last_error else "æœªçŸ¥éŒ¯èª¤"
        )
    
    async def _execute_scraping(self, url: str, start_time: float) -> PageContent:
        """åŸ·è¡Œå¯¦éš›çš„ç¶²é çˆ¬å–ä½œæ¥­ã€‚
        
        Args:
            url: è¦çˆ¬å–çš„ URL
            start_time: é–‹å§‹æ™‚é–“ (ç”¨æ–¼è¨ˆç®—è¼‰å…¥æ™‚é–“)
            
        Returns:
            PageContent: çˆ¬å–çµæœ
            
        Raises:
            å„ç¨®ç¶²è·¯å’Œè§£æç›¸é—œä¾‹å¤–
        """
        # å»ºç«‹ HTTP å®¢æˆ¶ç«¯é…ç½®
        timeout = aiohttp.ClientTimeout(total=self.timeout)
        headers = {
            'User-Agent': self.user_agents[hash(url) % len(self.user_agents)],
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-TW,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        
        async with aiohttp.ClientSession(timeout=timeout, headers=headers) as session:
            async with session.get(url) as response:
                # è¨˜éŒ„è¼‰å…¥æ™‚é–“
                load_time = time.time() - start_time
                status_code = response.status
                
                # æª¢æŸ¥å›æ‡‰ç‹€æ…‹
                if status_code >= 400:
                    return PageContent(
                        url=url,
                        h2_list=[],
                        status_code=status_code,
                        load_time=load_time,
                        success=False,
                        error=f"HTTP {status_code} éŒ¯èª¤"
                    )
                
                # è®€å–ç¶²é å…§å®¹
                html_content = await response.text()
                
                # è§£æ HTML ä¸¦æå– SEO å…ƒç´ 
                page_data = self._extract_seo_elements(html_content, url)
                
                return PageContent(
                    url=url,
                    h2_list=page_data.get('h2_list', []),
                    title=page_data.get('title'),
                    meta_description=page_data.get('meta_description'),
                    h1=page_data.get('h1'),
                    word_count=page_data.get('word_count', 0),
                    paragraph_count=page_data.get('paragraph_count', 0),
                    status_code=status_code,
                    load_time=load_time,
                    success=True
                )
    
    def _extract_seo_elements(self, html: str, _original_url: str) -> Dict[str, Any]:
        """å¾ HTML å…§å®¹ä¸­æå– SEO ç›¸é—œå…ƒç´ ã€‚
        
        Args:
            html: HTML åŸå§‹å…§å®¹
            original_url: åŸå§‹ URL (ç”¨æ–¼ç›¸å°é€£çµè™•ç†)
            
        Returns:
            dict: åŒ…å« SEO å…ƒç´ çš„å­—å…¸
            
        Raises:
            ScraperParsingException: HTML è§£æå¤±æ•—
        """
        try:
            # ä½¿ç”¨ lxml è§£æå™¨æå‡æ•ˆèƒ½
            soup = BeautifulSoup(html, 'lxml')
        except Exception as e:
            raise ScraperParsingException(f"HTML è§£æå¤±æ•—: {str(e)}")
        
        result = {}
        
        # æå–é é¢æ¨™é¡Œ
        title_tag = soup.find('title')
        result['title'] = title_tag.get_text(strip=True) if title_tag else None
        
        # æå– Meta Description
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if not meta_desc:
            meta_desc = soup.find('meta', attrs={'property': 'og:description'})
        
        if meta_desc and isinstance(meta_desc, Tag):
            # æ ¹æ“š BeautifulSoup æ–‡æª”ï¼Œä½¿ç”¨ get æ–¹æ³•å®‰å…¨å­˜å–å±¬æ€§
            content = meta_desc.get('content', '')
            if content and isinstance(content, (str, list)):
                # è™•ç†å¯èƒ½çš„ list é¡å‹ (å¦‚ class å±¬æ€§)
                content_str = content[0] if isinstance(content, list) else content
                result['meta_description'] = content_str.strip() if content_str else None
            else:
                result['meta_description'] = None
        else:
            result['meta_description'] = None
        
        # æå– H1 æ¨™ç±¤ (å–ç¬¬ä¸€å€‹)
        h1_tag = soup.find('h1')
        result['h1'] = h1_tag.get_text(strip=True) if h1_tag else None
        
        # æå–æ‰€æœ‰ H2 æ¨™ç±¤
        h2_tags = soup.find_all('h2')
        result['h2_list'] = []
        for h2 in h2_tags:
            if h2 and isinstance(h2, Tag):
                h2_text = h2.get_text(strip=True)
                if h2_text:
                    result['h2_list'].append(h2_text)
        
        # è¨ˆç®—å…§æ–‡å­—æ•¸å’Œæ®µè½æ•¸
        # ç§»é™¤ script, style, nav, footer ç­‰éå…§å®¹æ¨™ç±¤
        for element in soup(['script', 'style', 'nav', 'footer', 'header', 'aside']):
            element.decompose()
        
        # æå–ä¸»è¦å…§å®¹å€åŸŸçš„æ–‡å­—
        main_content = (soup.find('main') or soup.find('article') or 
                       soup.find('div', class_='content') or soup.find('body'))
        
        if main_content and isinstance(main_content, (Tag, NavigableString)):
            # è¨ˆç®—å­—æ•¸ (ç§»é™¤å¤šé¤˜ç©ºç™½)
            text_content = main_content.get_text(separator=' ', strip=True)
            # ä¸­è‹±æ–‡å­—æ•¸çµ±è¨ˆ (ä¸­æ–‡å­—ç¬¦ + è‹±æ–‡å–®å­—)
            chinese_chars = len([c for c in text_content if '\u4e00' <= c <= '\u9fff'])
            english_words = len(text_content.split()) - chinese_chars
            result['word_count'] = chinese_chars + english_words
            
            # è¨ˆç®—æ®µè½æ•¸ (p æ¨™ç±¤ + br æ¨™ç±¤çµ„)
            paragraphs = []
            if isinstance(main_content, Tag):
                try:
                    paragraphs = main_content.find_all('p')
                except Exception:
                    paragraphs = []
            
            if isinstance(main_content, Tag):
                br_groups = len(main_content.get_text().split('\n\n'))
                result['paragraph_count'] = max(len(paragraphs), br_groups - 1, 1)
            else:
                result['paragraph_count'] = 1
        else:
            result['word_count'] = 0
            result['paragraph_count'] = 0
        
        return result



# å…¨åŸŸæœå‹™å¯¦ä¾‹
_SCRAPER_SERVICE = None


def get_scraper_service() -> ScraperService:
    """å–å¾—çˆ¬èŸ²æœå‹™çš„å…¨åŸŸå¯¦ä¾‹ã€‚
    
    å¯¦ä½œå–®ä¾‹æ¨¡å¼ï¼Œç¢ºä¿æ•´å€‹æ‡‰ç”¨ç¨‹å¼ä½¿ç”¨åŒä¸€å€‹æœå‹™å¯¦ä¾‹ã€‚
    
    Returns:
        ScraperService: çˆ¬èŸ²æœå‹™å¯¦ä¾‹
    """
    global _SCRAPER_SERVICE
    if _SCRAPER_SERVICE is None:
        _SCRAPER_SERVICE = ScraperService()
    return _SCRAPER_SERVICE
