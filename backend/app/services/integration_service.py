"""æ•´åˆæœå‹™æ¨¡çµ„ã€‚

æ­¤æ¨¡çµ„æä¾›å„å€‹æœå‹™é–“çš„è³‡æ–™è½‰æ›ã€æ•´åˆå”èª¿å’Œçµ±ä¸€éŒ¯èª¤è™•ç†åŠŸèƒ½ã€‚
è² è²¬å°‡ SERPã€çˆ¬èŸ²ã€AI æœå‹™æ•´åˆç‚ºå®Œæ•´çš„ SEO åˆ†ææµç¨‹ã€‚
"""

import time
from datetime import datetime, timezone
from typing import Dict, List, Optional

from .job_manager import JobManager

from ..models.request import AnalyzeRequest, AnalyzeOptions as RequestOptions
from ..models.response import (
    AnalyzeResponse, AnalysisData, SerpSummary, 
    AnalysisMetadata
)
from .serp_service import get_serp_service, SerpResult, SerpAPIException
from .scraper_service import get_scraper_service, ScrapingResult, ScraperException
from .ai_service import (
    get_ai_service, AnalysisOptions as AIOptions, AnalysisResult,
    AIServiceException, TokenLimitExceededException, AIAPIException
)


class IntegrationService:
    """æ•´åˆæœå‹™é¡åˆ¥ã€‚
    
    å”èª¿ SERPã€çˆ¬èŸ²ã€AI æœå‹™çš„å®Œæ•´åˆ†ææµç¨‹ï¼Œ
    æä¾›çµ±ä¸€çš„è³‡æ–™è½‰æ›å’ŒéŒ¯èª¤è™•ç†æ©Ÿåˆ¶ã€‚
    """

    def __init__(self):
        """åˆå§‹åŒ–æ•´åˆæœå‹™ã€‚"""
        self.serp_service = get_serp_service()
        self.scraper_service = get_scraper_service()
        self.ai_service = get_ai_service()
        
        # æ•ˆèƒ½ç›£æ§é…ç½®
        self.performance_thresholds = {
            "serp_duration": 15.0,      # SERP éšæ®µè­¦å‘Šé–¾å€¼
            "scraping_duration": 25.0,  # çˆ¬èŸ²éšæ®µè­¦å‘Šé–¾å€¼
            "ai_duration": 35.0,        # AI éšæ®µè­¦å‘Šé–¾å€¼
            "total_duration": 55.0      # ç¸½æ™‚é–“è­¦å‘Šé–¾å€¼
        }
    
    async def execute_full_analysis(self, request: AnalyzeRequest) -> AnalyzeResponse:
        """åŸ·è¡Œå®Œæ•´çš„ SEO åˆ†ææµç¨‹ã€‚
        
        æ•´åˆ SERP æ“·å–ã€ç¶²é çˆ¬å–å’Œ AI åˆ†æï¼Œç”Ÿæˆå®Œæ•´çš„åˆ†æå ±å‘Šã€‚
        
        Args:
            request: SEO åˆ†æè«‹æ±‚
            
        Returns:
            AnalyzeResponse: å®Œæ•´çš„åˆ†æçµæœ
            
        Raises:
            å„ç¨®æœå‹™ç›¸é—œä¾‹å¤–
        """
        start_time = time.time()
        timer = PerformanceTimer()
        
        try:
            # éšæ®µ 1: SERP è³‡æ–™æ“·å–
            print(f"ğŸ” é–‹å§‹ SERP è³‡æ–™æ“·å–: {request.keyword}")
            timer.start_phase("serp")
            
            serp_data = await self.serp_service.search_keyword(
                keyword=request.keyword,
                num_results=10
            )
            
            timer.end_phase("serp")
            print(f"âœ… SERP æ“·å–å®Œæˆï¼Œå–å¾— {len(serp_data.organic_results)} å€‹çµæœ "
                  f"({timer.get_phase_duration('serp'):.2f}s)")
            
            # å°å‡º SERP è³‡æ–™å…§å®¹
            print("ğŸ“‹ SERP æ“·å–è³‡æ–™å…§å®¹ï¼š")
            for i, result in enumerate(serp_data.organic_results, 1):
                print(f"  {i}. {result.title[:100]}{'...' if len(result.title) > 100 else ''}")
                print(f"     URL: {result.link}")
                if result.snippet:
                    print(f"     æ‘˜è¦: {result.snippet[:200]}{'...' if len(result.snippet) > 200 else ''}")
                print()
            
            # éšæ®µ 2: ç¶²é å…§å®¹çˆ¬å–
            print("ğŸ•·ï¸ é–‹å§‹ç¶²é å…§å®¹çˆ¬å–")
            timer.start_phase("scraping")
            
            urls = self._extract_urls_from_serp(serp_data)
            scraping_data = await self.scraper_service.scrape_urls(urls)
            
            timer.end_phase("scraping")
            success_rate = scraping_data.successful_scrapes / scraping_data.total_results
            print(f"âœ… ç¶²é çˆ¬å–å®Œæˆï¼ŒæˆåŠŸç‡ {success_rate:.1%} "
                  f"({scraping_data.successful_scrapes}/{scraping_data.total_results}) "
                  f"({timer.get_phase_duration('scraping'):.2f}s)")
            
            # æª¢æŸ¥çˆ¬å–æˆåŠŸç‡
            if success_rate < 0.5:  # ä½æ–¼ 50% å‰‡è­¦å‘Š
                print(f"âš ï¸ çˆ¬å–æˆåŠŸç‡è¼ƒä½: {success_rate:.1%}")
            
            # éšæ®µ 3: AI åˆ†æå ±å‘Šç”Ÿæˆ
            print("ğŸ¤– é–‹å§‹ AI åˆ†æå ±å‘Šç”Ÿæˆ")
            timer.start_phase("ai")
            
            ai_options = self._convert_to_ai_options(request.options)
            analysis_result = await self.ai_service.analyze_seo_content(
                keyword=request.keyword,
                audience=request.audience,
                serp_data=serp_data,
                scraping_data=scraping_data,
                options=ai_options
            )
            
            timer.end_phase("ai")
            print(f"âœ… AI åˆ†æå®Œæˆï¼Œä½¿ç”¨ {analysis_result.token_usage} tokens "
                  f"({timer.get_phase_duration('ai'):.2f}s)")
            
            # éšæ®µ 4: çµæœæ•´åˆ
            total_time = time.time() - start_time
            print(f"ğŸ“‹ æ•´åˆåˆ†æçµæœï¼Œç¸½è€—æ™‚ {total_time:.2f}s")
            
            response = self._build_success_response(
                request=request,
                serp_data=serp_data,
                scraping_data=scraping_data,
                analysis_result=analysis_result,
                processing_time=total_time,
                timer=timer
            )
            
            # æ•ˆèƒ½è­¦å‘Šæª¢æŸ¥
            self._check_performance_warnings(timer)
            
            return response
            
        except Exception as e:
            processing_time = time.time() - start_time
            print(f"âŒ åˆ†ææµç¨‹å¤±æ•—: {str(e)} (è€—æ™‚ {processing_time:.2f}s)")
            raise e  # é‡æ–°æ‹‹å‡ºä¾‹å¤–ï¼Œç”±ä¸Šå±¤è™•ç†
    
    def _extract_urls_from_serp(self, serp_data: SerpResult) -> List[str]:
        """å¾ SERP è³‡æ–™ä¸­æå– URL æ¸…å–®ã€‚
        
        Args:
            serp_data: SERP æœå°‹çµæœ
            
        Returns:
            List[str]: URL æ¸…å–®
        """
        urls = []
        for result in serp_data.organic_results:
            if result.link and result.link.startswith(('http://', 'https://')):
                urls.append(result.link)
        
        print(f"ğŸ“ å¾ SERP æå– {len(urls)} å€‹æœ‰æ•ˆ URL")
        return urls
    
    def _convert_to_ai_options(self, request_options: RequestOptions) -> AIOptions:
        """è½‰æ›è«‹æ±‚é¸é …ç‚º AI æœå‹™é¸é …ã€‚
        
        Args:
            request_options: è«‹æ±‚ä¸­çš„é¸é …
            
        Returns:
            AIOptions: AI æœå‹™é¸é …
        """
        return AIOptions(
            generate_draft=request_options.generate_draft,
            include_faq=request_options.include_faq,
            include_table=request_options.include_table
        )
    
    def _build_success_response(
        self,
        request: AnalyzeRequest,
        serp_data: SerpResult,
        scraping_data: ScrapingResult,
        analysis_result: AnalysisResult,
        processing_time: float,
        timer: Optional['PerformanceTimer'] = None
    ) -> AnalyzeResponse:
        """å»ºç«‹æˆåŠŸå›æ‡‰ã€‚
        
        Args:
            request: åŸå§‹è«‹æ±‚
            serp_data: SERP è³‡æ–™
            scraping_data: çˆ¬èŸ²è³‡æ–™
            analysis_result: AI åˆ†æçµæœ
            processing_time: ç¸½è™•ç†æ™‚é–“
            
        Returns:
            AnalyzeResponse: å®Œæ•´çš„æˆåŠŸå›æ‡‰
        """
        # å»ºç«‹ SERP æ‘˜è¦
        serp_summary = SerpSummary(
            total_results=scraping_data.total_results,
            successful_scrapes=scraping_data.successful_scrapes,
            avg_word_count=scraping_data.avg_word_count,
            avg_paragraphs=scraping_data.avg_paragraphs
        )
        
        # å»ºç«‹åˆ†æå…ƒè³‡æ–™ (åŒ…å«éšæ®µè¨ˆæ™‚è³‡è¨Š)
        metadata_dict = {
            "keyword": request.keyword,
            "audience": request.audience,
            "generated_at": datetime.now(timezone.utc).isoformat(),
            "token_usage": analysis_result.token_usage
        }
        
        # æ·»åŠ éšæ®µè¨ˆæ™‚è³‡è¨Š (å¦‚æœæœ‰çš„è©±)
        if timer:
            phase_timings = timer.get_all_timings()
            if phase_timings:
                metadata_dict["phase_timings"] = phase_timings
                metadata_dict["total_phases_time"] = sum(phase_timings.values())
        
        metadata = AnalysisMetadata(**metadata_dict)
        
        # å»ºç«‹åˆ†æè³‡æ–™
        data = AnalysisData(
            serp_summary=serp_summary,
            analysis_report=analysis_result.analysis_report,
            metadata=metadata
        )
        
        # å»ºç«‹å®Œæ•´å›æ‡‰
        return AnalyzeResponse(
            status="success",
            processing_time=processing_time,
            data=data
        )
    
    
    def _check_performance_warnings(self, timer: 'PerformanceTimer') -> None:
        """æª¢æŸ¥æ•ˆèƒ½è­¦å‘Šã€‚
        
        Args:
            timer: æ•ˆèƒ½è¨ˆæ™‚å™¨
        """
        timings = timer.get_all_timings()
        
        for phase, duration in timings.items():
            if phase.endswith('_duration'):
                phase_name = phase.replace('_duration', '')
                threshold = self.performance_thresholds.get(phase, float('inf'))
                
                if duration > threshold:
                    print(f"âš ï¸ æ•ˆèƒ½è­¦å‘Š: {phase_name} éšæ®µè€—æ™‚ {duration:.2f}s "
                          f"(è¶…é {threshold}s é–¾å€¼)")

    async def execute_full_analysis_with_progress(
        self,
        request: AnalyzeRequest,
        job_manager: 'JobManager',
        job_id: str
    ) -> AnalyzeResponse:
        """åŸ·è¡Œå®Œæ•´åˆ†ææµç¨‹ä¸¦è¿½è¹¤é€²åº¦ã€‚

        æ­¤æ–¹æ³•èˆ‡ execute_full_analysis ç›¸åŒï¼Œä½†æœƒæ›´æ–°ä»»å‹™é€²åº¦ã€‚

        Args:
            request: SEO åˆ†æè«‹æ±‚
            job_manager: ä»»å‹™ç®¡ç†å™¨
            job_id: ä»»å‹™è­˜åˆ¥ç¢¼

        Returns:
            AnalyzeResponse: å®Œæ•´çš„åˆ†æçµæœ

        Raises:
            å„ç¨®æœå‹™ç›¸é—œä¾‹å¤–
        """
        start_time = time.time()
        timer = PerformanceTimer()

        try:
            # éšæ®µ 1: SERP è³‡æ–™æ“·å–
            job_manager.update_progress(
                job_id, 1, "æ­£åœ¨æ“·å– SERP è³‡æ–™...", 10.0
            )
            print(f"ğŸ” é–‹å§‹ SERP è³‡æ–™æ“·å–: {request.keyword}")
            timer.start_phase("serp")

            serp_data = await self.serp_service.search_keyword(
                keyword=request.keyword,
                num_results=10
            )

            timer.end_phase("serp")
            job_manager.update_progress(
                job_id, 1, "SERP è³‡æ–™æ“·å–å®Œæˆ", 30.0
            )
            print(f"âœ… SERP æ“·å–å®Œæˆï¼Œå–å¾— {len(serp_data.organic_results)} å€‹çµæœ "
                  f"({timer.get_phase_duration('serp'):.2f}s)")
            
            # å°å‡º SERP è³‡æ–™å…§å®¹
            print("ğŸ“‹ SERP æ“·å–è³‡æ–™å…§å®¹ï¼š")
            for i, result in enumerate(serp_data.organic_results, 1):
                print(f"  {i}. {result.title[:100]}{'...' if len(result.title) > 100 else ''}")
                print(f"     URL: {result.link}")
                if result.snippet:
                    print(f"     æ‘˜è¦: {result.snippet[:200]}{'...' if len(result.snippet) > 200 else ''}")
                print()

            # éšæ®µ 2: ç¶²é å…§å®¹çˆ¬å–
            job_manager.update_progress(
                job_id, 2, "æ­£åœ¨çˆ¬å–ç¶²é å…§å®¹...", 35.0
            )
            print("ğŸ•·ï¸ é–‹å§‹ç¶²é å…§å®¹çˆ¬å–")
            timer.start_phase("scraping")

            scraping_data = await self.scraper_service.scrape_urls(
                urls=[result.link for result in serp_data.organic_results]
            )

            timer.end_phase("scraping")
            job_manager.update_progress(
                job_id, 2, "ç¶²é çˆ¬å–å®Œæˆ", 60.0
            )
            print(f"âœ… çˆ¬å–å®Œæˆï¼ŒæˆåŠŸç‡ {scraping_data.successful_scrapes}/"
                  f"{scraping_data.total_results} "
                  f"({timer.get_phase_duration('scraping'):.2f}s)")

            # éšæ®µ 3: AI åˆ†æ
            job_manager.update_progress(
                job_id, 3, "æ­£åœ¨é€²è¡Œ AI åˆ†æ...", 65.0
            )
            print("ğŸ¤– é–‹å§‹ AI åˆ†æ")
            timer.start_phase("ai")

            ai_options = self._convert_to_ai_options(request.options)
            
            # æš«æ™‚è¨»è§£ AI åˆ†æä»¥ä¾¿æ¸¬è©¦é€²åº¦é¡¯ç¤ºåŠŸèƒ½
            # analysis_result = await self.ai_service.analyze_seo_content(
            #     serp_data=serp_data,
            #     scraping_data=scraping_data,
            #     keyword=request.keyword,
            #     audience=request.audience,
            #     options=ai_options
            # )
            
            # ä½¿ç”¨æ¨¡æ“¬çš„ AI åˆ†æçµæœé€²è¡Œæ¸¬è©¦
            from ..services.ai_service import AnalysisResult
            import asyncio
            
            # æ¨¡æ“¬ AI è™•ç†æ™‚é–“ï¼ˆ5ç§’ï¼‰
            print("ğŸ¤– æ¨¡æ“¬ AI åˆ†æè™•ç†ä¸­...")
            await asyncio.sleep(5)
            
            analysis_result = AnalysisResult(
                analysis_report=f"""# SEO åˆ†æå ±å‘Š

## é—œéµå­—åˆ†æï¼š{request.keyword}
ç›®æ¨™å—çœ¾ï¼š{request.audience}

### SERP åˆ†æçµæœ
- å…±æ‰¾åˆ° {len(serp_data.organic_results)} å€‹æœå°‹çµæœ
- ç¶²é çˆ¬å–æˆåŠŸ {scraping_data.successful_scrapes} å€‹é é¢

### æ¨¡æ“¬åˆ†æå»ºè­°
1. **å…§å®¹å„ªåŒ–å»ºè­°**
   - é‡å°é—œéµå­— "{request.keyword}" å„ªåŒ–æ¨™é¡Œå’Œå…§å®¹
   - æå‡å…§å®¹ç›¸é—œæ€§å’Œæ¬Šå¨æ€§

2. **æŠ€è¡“å„ªåŒ–å»ºè­°**  
   - æ”¹å–„é é¢è¼‰å…¥é€Ÿåº¦
   - å„ªåŒ–è¡Œå‹•è£ç½®é«”é©—

*ï¼ˆæ­¤ç‚ºæ¸¬è©¦æ¨¡å¼çš„æ¨¡æ“¬å ±å‘Šï¼‰*
""",
                token_usage=1500,  # æ¨¡æ“¬ token ä½¿ç”¨é‡
                processing_time=5.0,  # æ¨¡æ“¬è™•ç†æ™‚é–“
                success=True
            )

            timer.end_phase("ai")
            job_manager.update_progress(
                job_id, 3, "AI åˆ†æå®Œæˆ", 95.0
            )
            print(f"âœ… AI åˆ†æå®Œæˆï¼Œä½¿ç”¨ {analysis_result.token_usage} tokens "
                  f"({timer.get_phase_duration('ai'):.2f}s)")

            # å»ºæ§‹å›æ‡‰
            processing_time = time.time() - start_time
            response = self._build_success_response(
                request=request,
                serp_data=serp_data,
                scraping_data=scraping_data,
                analysis_result=analysis_result,
                processing_time=processing_time,
                timer=timer
            )

            # æª¢æŸ¥æ•ˆèƒ½è­¦å‘Š
            self._check_performance_warnings(timer)

            job_manager.update_progress(
                job_id, 3, "åˆ†æå®Œæˆ", 100.0
            )
            return response

        except Exception as e:
            # ä»»å‹™å¤±æ•—æ™‚æ›´æ–°ç‹€æ…‹
            job_manager.fail_job(job_id, str(e))
            raise


class PerformanceTimer:
    """æ•ˆèƒ½è¨ˆæ™‚å™¨ã€‚
    
    ç”¨æ–¼ç›£æ§å„å€‹è™•ç†éšæ®µçš„åŸ·è¡Œæ™‚é–“ã€‚
    """
    
    def __init__(self):
        """åˆå§‹åŒ–è¨ˆæ™‚å™¨ã€‚"""
        self.timings = {}
    
    def start_phase(self, phase_name: str) -> None:
        """é–‹å§‹è¨ˆæ™‚ç‰¹å®šéšæ®µã€‚
        
        Args:
            phase_name: éšæ®µåç¨±
        """
        self.timings[f"{phase_name}_start"] = time.time()
    
    def end_phase(self, phase_name: str) -> None:
        """çµæŸè¨ˆæ™‚ç‰¹å®šéšæ®µã€‚
        
        Args:
            phase_name: éšæ®µåç¨±
        """
        start_key = f"{phase_name}_start"
        duration_key = f"{phase_name}_duration"
        
        if start_key in self.timings:
            start_time = self.timings[start_key]
            self.timings[duration_key] = time.time() - start_time
    
    def get_phase_duration(self, phase_name: str) -> float:
        """å–å¾—ç‰¹å®šéšæ®µçš„æŒçºŒæ™‚é–“ã€‚
        
        Args:
            phase_name: éšæ®µåç¨±
            
        Returns:
            float: æŒçºŒæ™‚é–“ï¼ˆç§’ï¼‰
        """
        return self.timings.get(f"{phase_name}_duration", 0.0)
    
    def get_all_timings(self) -> Dict[str, float]:
        """å–å¾—æ‰€æœ‰è¨ˆæ™‚è³‡è¨Šã€‚
        
        Returns:
            Dict[str, float]: æ‰€æœ‰è¨ˆæ™‚è³‡è¨Š
        """
        return {k: v for k, v in self.timings.items() if k.endswith('_duration')}
    
    def get_summary(self) -> Dict[str, float]:
        """å–å¾—è¨ˆæ™‚æ‘˜è¦ã€‚
        
        Returns:
            Dict[str, float]: è¨ˆæ™‚æ‘˜è¦
        """
        duration_timings = self.get_all_timings()
        total_duration = sum(duration_timings.values())
        
        return {
            **duration_timings,
            "total_duration": total_duration
        }


# å…¨åŸŸæœå‹™å¯¦ä¾‹
_integration_service = None


def get_integration_service() -> IntegrationService:
    """å–å¾—æ•´åˆæœå‹™çš„å…¨åŸŸå¯¦ä¾‹ã€‚
    
    å¯¦ä½œå–®ä¾‹æ¨¡å¼ï¼Œç¢ºä¿æ•´å€‹æ‡‰ç”¨ç¨‹å¼ä½¿ç”¨åŒä¸€å€‹æœå‹™å¯¦ä¾‹ã€‚
    
    Returns:
        IntegrationService: æ•´åˆæœå‹™å¯¦ä¾‹
    """
    global _integration_service
    if _integration_service is None:
        _integration_service = IntegrationService()
    return _integration_service