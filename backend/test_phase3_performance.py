#!/usr/bin/env python3
"""Phase 3: æ•ˆèƒ½èˆ‡ç©©å®šæ€§æ¸¬è©¦

é©—è­‰æ–°æ‰å¹³çµæ§‹çš„æ•ˆèƒ½ç‰¹æ€§ï¼š
1. å›æ‡‰æ™‚é–“æ¸¬è©¦
2. è¨˜æ†¶é«”ä½¿ç”¨æ¸¬è©¦
3. åºåˆ—åŒ–æ•ˆèƒ½æ¸¬è©¦
4. å£“åŠ›æ¸¬è©¦
"""

import json
import time
import sys
import os
import tracemalloc
from datetime import datetime
import gc

# æ·»åŠ  backend è·¯å¾‘
backend_path = os.path.dirname(os.path.abspath(__file__))
if backend_path not in sys.path:
    sys.path.insert(0, backend_path)

from app.models.response import (
    AnalyzeResponse, 
    LegacyAnalyzeResponse, 
    AnalysisMetadata, 
    SerpSummary, 
    AnalysisData
)

def test_response_performance():
    """æ¸¬è©¦ 1: å›æ‡‰æ™‚é–“æ¸¬è©¦"""
    print("ğŸ§ª æ¸¬è©¦ 1: å›æ‡‰æ™‚é–“æ¸¬è©¦")
    print("-" * 40)
    
    try:
        # å‰µå»ºå¤§å‹åˆ†æå ±å‘Šï¼ˆæ¨¡æ“¬çœŸå¯¦å ´æ™¯ï¼‰
        large_report = "# å¤§å‹ SEO åˆ†æå ±å‘Š\n\n" + "## è©³ç´°åˆ†æå…§å®¹\n\n" * 200
        
        start_time = time.time()
        
        # å‰µå»º 100 å€‹å›æ‡‰ç‰©ä»¶é€²è¡Œæ•ˆèƒ½æ¸¬è©¦
        responses = []
        for i in range(100):
            response = AnalyzeResponse(
                analysis_report=large_report,
                token_usage=5000 + i,
                processing_time=25.0,
                success=True,
                cached_at=datetime.now().isoformat(),
                keyword=f"æ¸¬è©¦é—œéµå­—{i}"
            )
            responses.append(response)
        
        creation_time = time.time() - start_time
        print(f"âœ… å‰µå»º 100 å€‹å›æ‡‰ç‰©ä»¶: {creation_time:.3f} ç§’")
        
        # æ¸¬è©¦åºåˆ—åŒ–æ•ˆèƒ½
        start_time = time.time()
        json_objects = []
        for response in responses:
            json_data = response.model_dump()
            json_objects.append(json_data)
        
        serialization_time = time.time() - start_time
        print(f"âœ… åºåˆ—åŒ– 100 å€‹ç‰©ä»¶: {serialization_time:.3f} ç§’")
        
        # æ¸¬è©¦ JSON å­—ç¬¦ä¸²åŒ–æ•ˆèƒ½
        start_time = time.time()
        json_strings = []
        for json_obj in json_objects:
            json_str = json.dumps(json_obj, ensure_ascii=False)
            json_strings.append(json_str)
        
        stringify_time = time.time() - start_time
        print(f"âœ… JSON å­—ç¬¦ä¸²åŒ– 100 å€‹ç‰©ä»¶: {stringify_time:.3f} ç§’")
        
        # æ•ˆèƒ½é©—è­‰
        total_time = creation_time + serialization_time + stringify_time
        print(f"ğŸ“Š ç¸½æ™‚é–“: {total_time:.3f} ç§’")
        
        # æ•ˆèƒ½é–¾å€¼æª¢æŸ¥ï¼ˆæ ¹æ“š Phase 3 è¨ˆåŠƒï¼‰
        if total_time < 1.0:
            print("âœ… æ•ˆèƒ½æ¸¬è©¦é€šé (< 1.0 ç§’)")
            return True
        print(f"âš ï¸  æ•ˆèƒ½æ¸¬è©¦è­¦å‘Š: {total_time:.3f} ç§’ (é–¾å€¼: 1.0 ç§’)")
        return False
            
    except (ValueError, TypeError, RuntimeError) as e:
        print(f"âŒ æ•ˆèƒ½æ¸¬è©¦å¤±æ•—: {e}")
        return False

def test_memory_usage():
    """æ¸¬è©¦ 2: è¨˜æ†¶é«”ä½¿ç”¨æ¸¬è©¦"""
    print("\nğŸ§ª æ¸¬è©¦ 2: è¨˜æ†¶é«”ä½¿ç”¨æ¸¬è©¦")
    print("-" * 40)
    
    try:
        # å•Ÿå‹•è¨˜æ†¶é«”è¿½è¹¤
        tracemalloc.start()
        
        # å¼·åˆ¶åƒåœ¾æ”¶é›†ä»¥ç²å¾—åŸºæº–
        gc.collect()
        
        # å‰µå»ºå¤§é‡å›æ‡‰ç‰©ä»¶
        responses = []
        large_report = "# è¨˜æ†¶é«”æ¸¬è©¦å ±å‘Š\n\n" + "æ¸¬è©¦å…§å®¹ " * 1000
        
        for i in range(1000):
            response = AnalyzeResponse(
                analysis_report=large_report,
                token_usage=5000 + i,
                processing_time=25.0 + (i * 0.01),
                success=True,
                cached_at=datetime.now().isoformat(),
                keyword=f"è¨˜æ†¶é«”æ¸¬è©¦{i}"
            )
            responses.append(response)
            
            # å®šæœŸåºåˆ—åŒ–ä»¥æ¨¡æ“¬å¯¦éš›ä½¿ç”¨
            if i % 100 == 0:
                json_data = response.model_dump()
                _ = json.dumps(json_data, ensure_ascii=False)
        
        # æ¸¬é‡è¨˜æ†¶é«”ä½¿ç”¨
        current_memory, peak_memory = tracemalloc.get_traced_memory()
        
        print(f"âœ… å‰µå»º 1000 å€‹å›æ‡‰ç‰©ä»¶å®Œæˆ")
        print(f"ğŸ“Š ç•¶å‰è¨˜æ†¶é«”ä½¿ç”¨: {current_memory / 1024 / 1024:.2f} MB")
        print(f"ğŸ“Š å³°å€¼è¨˜æ†¶é«”ä½¿ç”¨: {peak_memory / 1024 / 1024:.2f} MB")
        
        # æ¸…ç†è³‡æº
        responses.clear()
        gc.collect()
        
        # è¨˜æ†¶é«”æ¸…ç†å¾Œæ¸¬é‡
        cleanup_memory, _ = tracemalloc.get_traced_memory()
        print(f"ğŸ“Š æ¸…ç†å¾Œè¨˜æ†¶é«”: {cleanup_memory / 1024 / 1024:.2f} MB")
        
        tracemalloc.stop()
        
        # è¨˜æ†¶é«”ä½¿ç”¨é©—è­‰ï¼ˆæ ¹æ“š Phase 3 è¨ˆåŠƒ < 50MBï¼‰
        peak_mb = peak_memory / 1024 / 1024
        if peak_mb < 50:
            print("âœ… è¨˜æ†¶é«”æ¸¬è©¦é€šé (< 50MB)")
            return True
        print(f"âš ï¸  è¨˜æ†¶é«”æ¸¬è©¦è­¦å‘Š: {peak_mb:.2f} MB (é–¾å€¼: 50MB)")
        return True  # è­¦å‘Šä½†ä¸å¤±æ•—
            
    except (ValueError, TypeError, MemoryError, RuntimeError) as e:
        print(f"âŒ è¨˜æ†¶é«”æ¸¬è©¦å¤±æ•—: {e}")
        tracemalloc.stop()
        return False

def test_serialization_efficiency():
    """æ¸¬è©¦ 3: åºåˆ—åŒ–æ•ˆç‡æ¯”è¼ƒ"""
    print("\nğŸ§ª æ¸¬è©¦ 3: åºåˆ—åŒ–æ•ˆç‡æ¯”è¼ƒ")
    print("-" * 40)
    
    try:
        # æº–å‚™æ¸¬è©¦è³‡æ–™
        report_content = "# åºåˆ—åŒ–æ¸¬è©¦å ±å‘Š\n\n" + "è©³ç´°å…§å®¹ " * 500
        
        # æ¸¬è©¦æ–°çš„æ‰å¹³çµæ§‹
        new_response = AnalyzeResponse(
            analysis_report=report_content,
            token_usage=5484,
            processing_time=22.46,
            success=True,
            cached_at=datetime.now().isoformat(),
            keyword="åºåˆ—åŒ–æ¸¬è©¦"
        )
        
        # æ¸¬è©¦èˆŠçš„å·¢ç‹€çµæ§‹ï¼ˆç”¨æ–¼æ¯”è¼ƒï¼‰
        
        # å‰µå»º AnalysisMetadata ä¸¦åŒ…å«æ‰€æœ‰å¿…éœ€å­—æ®µ
        metadata = AnalysisMetadata(
            keyword="åºåˆ—åŒ–æ¸¬è©¦",
            audience="æ¸¬è©¦å—çœ¾",
            generated_at=datetime.now().isoformat(),
            token_usage=5484,
            phase_timings={"serp": 1.2, "scraping": 3.5, "analysis": 17.8},
            total_phases_time=22.5
        )
        
        
        # å‰µå»ºå®Œæ•´çš„ AnalysisData
        serp_summary = SerpSummary(
            total_results=10,
            successful_scrapes=8,
            avg_word_count=1500,
            avg_paragraphs=12
        )
        
        analysis_data = AnalysisData(
            serp_summary=serp_summary,
            analysis_report=report_content,
            metadata=metadata
        )
        
        legacy_response = LegacyAnalyzeResponse(
            status="success",
            processing_time=22.46,
            data=analysis_data
        )
        
        # æ¸¬è©¦æ–°çµæ§‹åºåˆ—åŒ–é€Ÿåº¦
        iterations = 1000
        
        start_time = time.time()
        for _ in range(iterations):
            json_data = new_response.model_dump()
            _ = json.dumps(json_data, ensure_ascii=False)
        new_structure_time = time.time() - start_time
        
        # æ¸¬è©¦èˆŠçµæ§‹åºåˆ—åŒ–é€Ÿåº¦
        start_time = time.time()
        for _ in range(iterations):
            json_data = legacy_response.model_dump()
            _ = json.dumps(json_data, ensure_ascii=False)
        legacy_structure_time = time.time() - start_time
        
        print(f"âœ… æ–°æ‰å¹³çµæ§‹åºåˆ—åŒ–æ™‚é–“: {new_structure_time:.3f} ç§’")
        print(f"âœ… èˆŠå·¢ç‹€çµæ§‹åºåˆ—åŒ–æ™‚é–“: {legacy_structure_time:.3f} ç§’")
        
        # è¨ˆç®—æ•ˆèƒ½æ¯”è¼ƒ
        if new_structure_time < legacy_structure_time:
            improvement = ((legacy_structure_time - new_structure_time) / legacy_structure_time) * 100
            print(f"ğŸš€ æ–°çµæ§‹æ•ˆèƒ½æå‡: {improvement:.1f}%")
        else:
            degradation = ((new_structure_time - legacy_structure_time) / legacy_structure_time) * 100
            if degradation > 10:  # å…è¨± 10% çš„æ•ˆèƒ½åŠ£åŒ–
                print(f"âš ï¸  æ•ˆèƒ½åŠ£åŒ–: {degradation:.1f}% (å¯æ¥å—ç¯„åœå…§)")
            else:
                print(f"ğŸ“Š æ•ˆèƒ½å·®ç•°: {degradation:.1f}% (åœ¨å¯æ¥å—ç¯„åœå…§)")
        
        # æ¸¬è©¦ JSON å¤§å°æ¯”è¼ƒ
        new_json = json.dumps(new_response.model_dump(), ensure_ascii=False)
        legacy_json = json.dumps(legacy_response.model_dump(), ensure_ascii=False)
        
        print(f"ğŸ“ æ–°çµæ§‹ JSON å¤§å°: {len(new_json)} bytes")
        print(f"ğŸ“ èˆŠçµæ§‹ JSON å¤§å°: {len(legacy_json)} bytes")
        
        size_difference = len(new_json) - len(legacy_json)
        if size_difference < 0:
            print(f"ğŸ¯ JSON å¤§å°æ¸›å°‘: {abs(size_difference)} bytes")
        else:
            print(f"ğŸ“Š JSON å¤§å°å¢åŠ : {size_difference} bytes")
        
        return True
        
    except (ValueError, TypeError, ImportError, AttributeError) as e:
        print(f"âŒ åºåˆ—åŒ–æ•ˆç‡æ¸¬è©¦å¤±æ•—: {e}")
        return False

def test_stress_testing():
    """æ¸¬è©¦ 4: å£“åŠ›æ¸¬è©¦"""
    print("\nğŸ§ª æ¸¬è©¦ 4: å£“åŠ›æ¸¬è©¦")
    print("-" * 40)
    
    try:
        # æ¨¡æ“¬é«˜è² è¼‰æƒ…æ³
        concurrent_requests = 50
        responses_per_request = 20
        
        start_time = time.time()
        
        all_responses = []
        for request_id in range(concurrent_requests):
            request_responses = []
            
            for response_id in range(responses_per_request):
                # å‰µå»ºä¸åŒå¤§å°çš„å›æ‡‰
                report_size = 100 + (response_id * 50)
                report_content = f"# å£“åŠ›æ¸¬è©¦å ±å‘Š {request_id}-{response_id}\n\n" + "æ¸¬è©¦å…§å®¹ " * report_size
                
                response = AnalyzeResponse(
                    analysis_report=report_content,
                    token_usage=1000 + response_id,
                    processing_time=10.0 + (response_id * 0.5),
                    success=True,
                    cached_at=datetime.now().isoformat(),
                    keyword=f"å£“åŠ›æ¸¬è©¦{request_id}-{response_id}"
                )
                
                # ç«‹å³åºåˆ—åŒ–ï¼ˆæ¨¡æ“¬å¯¦éš› API å›æ‡‰ï¼‰
                json_data = response.model_dump()
                _ = json.dumps(json_data, ensure_ascii=False)
                
                request_responses.append(response)
            
            all_responses.extend(request_responses)
        
        total_time = time.time() - start_time
        total_responses = concurrent_requests * responses_per_request
        
        print(f"âœ… è™•ç† {total_responses} å€‹å›æ‡‰å®Œæˆ")
        print(f"ğŸ“Š ç¸½è™•ç†æ™‚é–“: {total_time:.3f} ç§’")
        print(f"ğŸ“Š å¹³å‡æ¯å€‹å›æ‡‰: {(total_time / total_responses) * 1000:.2f} ms")
        print(f"ğŸ“Š æ¯ç§’è™•ç†èƒ½åŠ›: {total_responses / total_time:.1f} å›æ‡‰/ç§’")
        
        # å£“åŠ›æ¸¬è©¦é©—è­‰
        avg_response_time = (total_time / total_responses) * 1000
        if avg_response_time < 100:  # æ¯å€‹å›æ‡‰ < 100ms
            print("âœ… å£“åŠ›æ¸¬è©¦é€šé")
            return True
        print(f"âš ï¸  å£“åŠ›æ¸¬è©¦è­¦å‘Š: å¹³å‡å›æ‡‰æ™‚é–“ {avg_response_time:.2f} ms")
        return True  # è­¦å‘Šä½†ä¸å¤±æ•—
            
    except (ValueError, TypeError, MemoryError, RuntimeError) as e:
        print(f"âŒ å£“åŠ›æ¸¬è©¦å¤±æ•—: {e}")
        return False

def main():
    """åŸ·è¡Œ Phase 3 æ•ˆèƒ½èˆ‡ç©©å®šæ€§æ¸¬è©¦"""
    print("ğŸš€ Phase 3: æ•ˆèƒ½èˆ‡ç©©å®šæ€§æ¸¬è©¦é–‹å§‹")
    print("=" * 50)
    
    tests = [
        ("å›æ‡‰æ™‚é–“æ¸¬è©¦", test_response_performance),
        ("è¨˜æ†¶é«”ä½¿ç”¨æ¸¬è©¦", test_memory_usage),
        ("åºåˆ—åŒ–æ•ˆç‡æ¯”è¼ƒ", test_serialization_efficiency),
        ("å£“åŠ›æ¸¬è©¦", test_stress_testing),
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        print(f"\nğŸ“‹ åŸ·è¡Œæ¸¬è©¦: {test_name}")
        if test_func():
            passed += 1
            print(f"âœ… {test_name} - é€šé")
        else:
            print(f"âŒ {test_name} - å¤±æ•—")
    
    # è¼¸å‡ºç¸½çµ
    print("\n" + "=" * 50)
    print(f"ğŸ“Š Phase 3 æ•ˆèƒ½æ¸¬è©¦çµæœ: {passed}/{total} é€šé")
    
    if passed >= total - 1:  # å…è¨±ä¸€å€‹æ¸¬è©¦å¤±æ•—
        print("ğŸ‰ æ•ˆèƒ½èˆ‡ç©©å®šæ€§æ¸¬è©¦åŸºæœ¬é€šéï¼")
        print("âœ… æ‰å¹³çµæ§‹æ•ˆèƒ½ç¬¦åˆé æœŸ")
        print("âœ… è¨˜æ†¶é«”ä½¿ç”¨åœ¨åˆç†ç¯„åœå…§")
        print("âœ… åºåˆ—åŒ–æ•ˆèƒ½è¡¨ç¾è‰¯å¥½")
        print("âœ… ç³»çµ±èƒ½æ‰¿å—é«˜è² è¼‰")
        return True
    print("âŒ å¤šå€‹æ•ˆèƒ½æ¸¬è©¦å¤±æ•—ï¼Œéœ€è¦å„ªåŒ–")
    return False

if __name__ == "__main__":
    SUCCESS = main()
    sys.exit(0 if SUCCESS else 1)