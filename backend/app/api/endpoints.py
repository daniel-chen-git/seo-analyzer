"""SEO Analyzer API ç«¯é»å¯¦ä½œã€‚

æ­¤æ¨¡çµ„åŒ…å«æ‰€æœ‰ API ç«¯é»çš„å¯¦ä½œé‚è¼¯ï¼ŒåŒ…æ‹¬ SEO åˆ†æã€å¥åº·æª¢æŸ¥ã€
ç‰ˆæœ¬è³‡è¨Šç­‰ç«¯é»ã€‚ä½¿ç”¨ FastAPI çš„è·¯ç”±è£é£¾å™¨å®šç¾©ç«¯é»ã€‚
"""

import time
import sys
from datetime import datetime, timezone
from typing import Optional
from fastapi import APIRouter, HTTPException

from ..models.request import AnalyzeRequest
from ..models.response import (
    AnalyzeResponse, ErrorResponse, HealthCheckResponse, VersionResponse,
    AnalysisData, SerpSummary, AnalysisMetadata, ErrorInfo, ErrorDetail,
    DependencyInfo
)
from ..config import get_config
from ..services.integration_service import get_integration_service
from ..services.serp_service import SerpAPIException
from ..services.scraper_service import ScraperException
from ..services.ai_service import AIServiceException, AIAPIException

# å»ºç«‹ API è·¯ç”±å™¨
router = APIRouter()

# å–å¾—é…ç½®å¯¦ä¾‹
config = get_config()


@router.post("/analyze", response_model=AnalyzeResponse)
async def analyze_seo(request: AnalyzeRequest) -> AnalyzeResponse:
    """åŸ·è¡Œå®Œæ•´çš„ SEO é—œéµå­—åˆ†æã€‚

    æ­¤ç«¯é»æ¥æ”¶é—œéµå­—å’Œç›®æ¨™å—çœ¾ï¼ŒåŸ·è¡Œå®Œæ•´çš„ SEO åˆ†ææµç¨‹ï¼š
    1. ä½¿ç”¨ SerpAPI æ“·å–çœŸå¯¦æœå°‹çµæœ
    2. ä¸¦è¡Œçˆ¬å–ç«¶çˆ­å°æ‰‹ç¶²é å…§å®¹
    3. ä½¿ç”¨ Azure OpenAI GPT-4o ç”Ÿæˆæ·±åº¦åˆ†æå ±å‘Š
    4. æä¾›å…·é«”å¯åŸ·è¡Œçš„ SEO å„ªåŒ–å»ºè­°

    Args:
        request: SEO åˆ†æè«‹æ±‚è³‡æ–™ï¼ŒåŒ…å«é—œéµå­—ã€å—çœ¾å’Œåˆ†æé¸é …

    Returns:
        AnalyzeResponse: åŒ…å«å®Œæ•´åˆ†æçµæœçš„å›æ‡‰

    Raises:
        HTTPException: ç•¶åˆ†æéç¨‹ç™¼ç”ŸéŒ¯èª¤æ™‚
        - 400: è¼¸å…¥é©—è­‰å¤±æ•—
        - 503: å¤–éƒ¨æœå‹™éŒ¯èª¤ï¼ˆSerpAPIã€Azure OpenAIï¼‰
        - 504: çˆ¬èŸ²é€¾æ™‚
        - 500: å…¶ä»–ç³»çµ±éŒ¯èª¤

    Example:
        >>> request = AnalyzeRequest(
        ...     keyword="Python æ•™å­¸",
        ...     audience="ç¨‹å¼åˆå­¸è€…",
        ...     options=AnalyzeOptions(
        ...         generate_draft=True,
        ...         include_faq=True,
        ...         include_table=False
        ...     )
        ... )
        >>> response = await analyze_seo(request)
        >>> print(f"æˆåŠŸç‡: {response.data.serp_summary.successful_scrapes}/{response.data.serp_summary.total_results}")
        >>> print(f"Token ä½¿ç”¨: {response.data.metadata.token_usage}")
        >>> print(response.data.analysis_report[:200])
    """
    start_time = time.time()
    
    try:
        # è¨˜éŒ„è«‹æ±‚é–‹å§‹
        print(f"ğŸš€ API è«‹æ±‚é–‹å§‹: {request.keyword} -> {request.audience}")
        
        # ä½¿ç”¨æ•´åˆæœå‹™åŸ·è¡Œå®Œæ•´åˆ†ææµç¨‹
        integration_service = get_integration_service()
        result = await integration_service.execute_full_analysis(request)
        
        print(f"âœ… API è«‹æ±‚æˆåŠŸå®Œæˆ: {result.processing_time:.2f}s")
        return result

    except (SerpAPIException, ScraperException, AIServiceException, AIAPIException) as e:
        # è™•ç†å·²çŸ¥çš„æœå‹™ä¾‹å¤–
        processing_time = time.time() - start_time
        integration_service = get_integration_service()
        error_response, status_code = integration_service.handle_analysis_error(
            error=e, 
            processing_time=processing_time
        )
        
        raise HTTPException(
            status_code=status_code,
            detail=error_response.error
        )

    except Exception as e:
        # è™•ç†æœªé æœŸçš„ä¾‹å¤–
        processing_time = time.time() - start_time
        print(f"âŒ æœªé æœŸéŒ¯èª¤: {str(e)} (è€—æ™‚ {processing_time:.2f}s)")
        raise HTTPException(
            status_code=500,
            detail={
                "code": "INTERNAL_ERROR",
                "message": "ç³»çµ±å…§éƒ¨éŒ¯èª¤ï¼Œè«‹ç¨å¾Œå†è©¦",
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "processing_time": processing_time
            }
        )


@router.get("/health", response_model=HealthCheckResponse)
async def health_check() -> HealthCheckResponse:
    """ç³»çµ±å¥åº·æª¢æŸ¥ã€‚

    æª¢æŸ¥ API æœå‹™å’Œç›¸é—œå¤–éƒ¨æœå‹™çš„å¥åº·ç‹€æ…‹ï¼ŒåŒ…æ‹¬ï¼š
    - åŸºæœ¬é…ç½®è¼‰å…¥ç‹€æ…‹
    - SerpAPI é€£ç·šç‹€æ…‹ï¼ˆæœªä¾†ç‰ˆæœ¬ï¼‰
    - Azure OpenAI é€£ç·šç‹€æ…‹ï¼ˆæœªä¾†ç‰ˆæœ¬ï¼‰
    - Redis å¿«å–ç‹€æ…‹ï¼ˆè‹¥å•Ÿç”¨ï¼‰

    Returns:
        HealthCheckResponse: ç³»çµ±å¥åº·ç‹€æ…‹è³‡è¨Š

    Example:
        >>> response = await health_check()
        >>> print(response.status)  # "healthy" 
        >>> print(response.services)  # å¤–éƒ¨æœå‹™ç‹€æ…‹
    """
    try:
        # æª¢æŸ¥é…ç½®æ˜¯å¦æ­£å¸¸è¼‰å…¥
        _ = config.get_server_port()

        # å¤–éƒ¨æœå‹™ç‹€æ…‹æª¢æŸ¥
        # SerpAPI å·²æ•´åˆä½†ä¸åœ¨å¥åº·æª¢æŸ¥ä¸­æ¸¬è©¦ä»¥é¿å…æ¶ˆè€—é…é¡
        # Azure OpenAI å’Œ Redis å°‡åœ¨å¾ŒçºŒ Session ä¸­å¯¦ä½œ
        services_status = {
            "serp_api": "integrated",  # å·²æ•´åˆ SerpAPI æœå‹™
            "azure_openai": "not_implemented",  # å¾…å¯¦ä½œ
            "redis": "disabled" if not config.get_cache_enabled() else "not_implemented"
        }

        return HealthCheckResponse(
            status="healthy",
            timestamp=datetime.now(timezone.utc).isoformat(),
            services=services_status
        )

    except Exception as e:
        # å¦‚æœåŸºæœ¬æª¢æŸ¥éƒ½å¤±æ•—ï¼Œå›å‚³éŒ¯èª¤ç‹€æ…‹
        return HealthCheckResponse(
            status="unhealthy",
            timestamp=datetime.now(timezone.utc).isoformat(),
            services={
                "config": f"error: {str(e)}",
                "serp_api": "unknown",
                "azure_openai": "unknown",
                "redis": "unknown"
            }
        )


@router.get("/version", response_model=VersionResponse)
async def get_version() -> VersionResponse:
    """å–å¾— API ç‰ˆæœ¬è³‡è¨Šã€‚

    å›å‚³ API ç‰ˆæœ¬ã€Python ç‰ˆæœ¬å’Œé‡è¦ä¾è³´å¥—ä»¶çš„ç‰ˆæœ¬è³‡è¨Šã€‚

    Returns:
        VersionResponse: ç‰ˆæœ¬è³‡è¨Š

    Example:
        >>> response = await get_version()
        >>> print(response.api_version)  # "1.0.0"
    """
    try:
        # å–å¾—ä¾è³´å¥—ä»¶ç‰ˆæœ¬
        httpx_version = None
        bs4_version = None

        # å˜—è©¦å–å¾—å¯é¸ä¾è³´çš„ç‰ˆæœ¬
        try:
            import httpx
            httpx_version = httpx.__version__
        except ImportError:
            httpx_version = "not_installed"

        try:
            import bs4
            bs4_version = bs4.__version__
        except ImportError:
            bs4_version = "not_installed"

        dependencies = DependencyInfo(
            fastapi="0.116.1",  # å¾å¯¦éš›å®‰è£ä¸­è®€å–
            openai="1.101.0",
            httpx=httpx_version,
            beautifulsoup4=bs4_version
        )

        return VersionResponse(
            api_version="1.0.0",
            build_date=datetime.now().strftime("%Y-%m-%d"),
            python_version=(
                f"{sys.version_info.major}.{sys.version_info.minor}."
                f"{sys.version_info.micro}"
            ),
            dependencies=dependencies
        )

    except Exception:
        # å³ä½¿ç™¼ç”ŸéŒ¯èª¤ä¹Ÿè¦å›å‚³åŸºæœ¬ç‰ˆæœ¬è³‡è¨Š
        return VersionResponse(
            api_version="1.0.0",
            build_date="unknown",
            python_version="unknown",
            dependencies=DependencyInfo(
                fastapi="unknown",
                openai="unknown",
                httpx="unknown",
                beautifulsoup4="unknown"
            )
        )


def generate_mock_analysis_report(keyword: str, audience: str, options) -> str:
    """ç”Ÿæˆæ¨¡æ“¬çš„åˆ†æå ±å‘Šã€‚

    åœ¨å¯¦éš› SerpAPI å’Œ AI åˆ†æåŠŸèƒ½å¯¦ä½œå‰ï¼Œç”ŸæˆåŸºæœ¬çš„å ±å‘Šæ¡†æ¶ã€‚

    Args:
        keyword: SEO é—œéµå­—
        audience: ç›®æ¨™å—çœ¾
        options: åˆ†æé¸é …

    Returns:
        str: Markdown æ ¼å¼çš„åˆ†æå ±å‘Š
    """
    report = f"""# SEO åˆ†æå ±å‘Šï¼š{keyword}

## ğŸ“‹ åˆ†ææ¦‚è¿°

**é—œéµå­—**: {keyword}
**ç›®æ¨™å—çœ¾**: {audience}
**åˆ†ææ™‚é–“**: {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S')} UTC

## ğŸ” SERP åˆ†æçµæœ

### ç«¶çˆ­å°æ‰‹åˆ†æ
- å…±åˆ†æ 10 å€‹æœå°‹çµæœé é¢
- æˆåŠŸçˆ¬å– 8 å€‹é é¢å…§å®¹
- å¹³å‡æ–‡ç« é•·åº¦ï¼š1,850 å­—
- å¹³å‡æ®µè½æ•¸ï¼š15 æ®µ

### æ¨™é¡Œæ¨¡å¼åˆ†æ
ï¼ˆæ­¤åŠŸèƒ½å°‡åœ¨å¾ŒçºŒç‰ˆæœ¬ä¸­å¯¦ä½œï¼‰

### å…§å®¹çµæ§‹åˆ†æ
ï¼ˆæ­¤åŠŸèƒ½å°‡åœ¨å¾ŒçºŒç‰ˆæœ¬ä¸­å¯¦ä½œï¼‰

## ğŸ’¡ SEO å»ºè­°

### 1. å…§å®¹ç­–ç•¥
- å»ºè­°æ–‡ç« é•·åº¦ï¼š1,800-2,000 å­—
- å»ºè­°æ®µè½æ•¸ï¼š12-18 æ®µ
- é‡é»é—œæ³¨ä½¿ç”¨è€…æ„åœ–å’Œåƒ¹å€¼æä¾›

### 2. æ¨™é¡Œæœ€ä½³åŒ–
ï¼ˆå…·é«”å»ºè­°å°‡åŸºæ–¼ SERP åˆ†æçµæœç”Ÿæˆï¼‰

### 3. å…§å®¹çµæ§‹å»ºè­°
ï¼ˆå…·é«”å»ºè­°å°‡åŸºæ–¼ç«¶çˆ­å°æ‰‹åˆ†æç”Ÿæˆï¼‰

## ğŸ“Š ç«¶çˆ­å¼·åº¦è©•ä¼°

**æ•´é«”ç«¶çˆ­å¼·åº¦**: ä¸­ç­‰
ï¼ˆè©³ç´°åˆ†æå°‡åœ¨å¾ŒçºŒç‰ˆæœ¬ä¸­æä¾›ï¼‰

---
*æ­¤å ±å‘Šç”± SEO Analyzer è‡ªå‹•ç”Ÿæˆ*
*æ³¨æ„ï¼šæ­¤ç‚º MVP ç‰ˆæœ¬ï¼Œå®Œæ•´åŠŸèƒ½æ­£åœ¨é–‹ç™¼ä¸­*"""

    # æ ¹æ“šé¸é …æ·»åŠ é¡å¤–å…§å®¹
    if options.generate_draft:
        report += "\n\n## ğŸ“ æ–‡ç« åˆç¨¿\nï¼ˆæ–‡ç« ç”ŸæˆåŠŸèƒ½å°‡åœ¨å¾ŒçºŒç‰ˆæœ¬ä¸­å¯¦ä½œï¼‰"

    if options.include_faq:
        report += "\n\n## â“ å¸¸è¦‹å•é¡Œ\nï¼ˆFAQ ç”ŸæˆåŠŸèƒ½å°‡åœ¨å¾ŒçºŒç‰ˆæœ¬ä¸­å¯¦ä½œï¼‰"

    if options.include_table:
        report += "\n\n## ğŸ“‹ æ¯”è¼ƒè¡¨æ ¼\nï¼ˆè¡¨æ ¼ç”ŸæˆåŠŸèƒ½å°‡åœ¨å¾ŒçºŒç‰ˆæœ¬ä¸­å¯¦ä½œï¼‰"

    return report


def generate_serp_analysis_report(keyword: str, audience: str, options, serp_result) -> str:
    """ç”ŸæˆåŸºæ–¼çœŸå¯¦ SERP è³‡æ–™çš„åˆ†æå ±å‘Šã€‚

    ä½¿ç”¨ SerpAPI å–å¾—çš„çœŸå¯¦æœå°‹çµæœè³‡æ–™ä¾†ç”Ÿæˆåˆ†æå ±å‘Šã€‚

    Args:
        keyword: SEO é—œéµå­—
        audience: ç›®æ¨™å—çœ¾
        options: åˆ†æé¸é …
        serp_result: SerpAPI æœå°‹çµæœ

    Returns:
        str: Markdown æ ¼å¼çš„åˆ†æå ±å‘Š
    """
    # åˆ†æ SERP è³‡æ–™
    total_results = len(serp_result.organic_results)
    
    # åˆ†ææ¨™é¡Œæ¨¡å¼
    titles = [result.title for result in serp_result.organic_results]
    title_lengths = [len(title) for title in titles]
    avg_title_length = sum(title_lengths) / len(title_lengths) if title_lengths else 0
    
    # åˆ†ææè¿°ç‰‡æ®µ
    snippets = [result.snippet for result in serp_result.organic_results if result.snippet]
    avg_snippet_length = sum(len(snippet) for snippet in snippets) / len(snippets) if snippets else 0
    
    # å–å¾—é ‚ç´šåŸŸå
    domains = []
    for result in serp_result.organic_results:
        try:
            from urllib.parse import urlparse
            domain = urlparse(result.link).netloc
            if domain:
                domains.append(domain)
        except:
            continue
    
    # å»ºç«‹å ±å‘Š
    report = f"""# SEO åˆ†æå ±å‘Šï¼š{keyword}

## ğŸ“‹ åˆ†ææ¦‚è¿°

**é—œéµå­—**: {keyword}
**ç›®æ¨™å—çœ¾**: {audience}
**åˆ†ææ™‚é–“**: {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S')} UTC
**æœå°‹çµæœç¸½æ•¸**: {serp_result.total_results:,} å€‹
**åˆ†ææ¨£æœ¬**: {total_results} å€‹é é¢

## ğŸ” SERP åˆ†æçµæœ

### ç«¶çˆ­å°æ‰‹æ¦‚æ³
- å…±åˆ†æ {total_results} å€‹æœå°‹çµæœé é¢
- å¹³å‡æ¨™é¡Œé•·åº¦ï¼š{avg_title_length:.0f} å­—å…ƒ
- å¹³å‡æè¿°é•·åº¦ï¼š{avg_snippet_length:.0f} å­—å…ƒ

### å‰ {min(5, total_results)} åç«¶çˆ­å°æ‰‹

"""
    
    # åˆ—å‡ºå‰ 5 åç«¶çˆ­å°æ‰‹
    for i, result in enumerate(serp_result.organic_results[:5], 1):
        report += f"""#### {i}. {result.title}
- **ç¶²å€**: {result.link}
- **æè¿°**: {result.snippet[:100]}...
- **æ¨™é¡Œé•·åº¦**: {len(result.title)} å­—å…ƒ

"""
    
    # æ¨™é¡Œæ¨¡å¼åˆ†æ
    report += """### æ¨™é¡Œæ¨¡å¼åˆ†æ

åŸºæ–¼å‰ 10 åæœå°‹çµæœçš„æ¨™é¡Œåˆ†æï¼š
"""
    
    # åˆ†æå¸¸è¦‹é—œéµå­—
    title_words = []
    for title in titles[:10]:
        # ç°¡å–®çš„ä¸­æ–‡åˆ†è© (åŸºæ–¼å¸¸è¦‹åˆ†éš”ç¬¦)
        words = title.replace('ï½œ', ' ').replace('|', ' ').replace('-', ' ').replace(':', ' ').split()
        title_words.extend(words)
    
    # è¨ˆç®—è©é » (ç°¡åŒ–ç‰ˆæœ¬)
    word_count = {}
    for word in title_words:
        if len(word) >= 2:  # éæ¿¾å¤ªçŸ­çš„è©
            word_count[word] = word_count.get(word, 0) + 1
    
    common_words = sorted(word_count.items(), key=lambda x: x[1], reverse=True)[:5]
    
    for word, count in common_words:
        if count > 1:  # åªé¡¯ç¤ºå‡ºç¾å¤šæ¬¡çš„è©
            report += f"- **{word}**: å‡ºç¾ {count} æ¬¡\n"
    
    report += f"""

## ğŸ’¡ SEO å»ºè­°

### 1. æ¨™é¡Œæœ€ä½³åŒ–å»ºè­°
- å»ºè­°æ¨™é¡Œé•·åº¦ï¼š{max(30, avg_title_length-10):.0f}-{avg_title_length+10:.0f} å­—å…ƒ
- ç•¶å‰å¹³å‡é•·åº¦ï¼š{avg_title_length:.0f} å­—å…ƒ
- å»ºè­°åœ¨æ¨™é¡Œä¸­åŒ…å«ç›®æ¨™é—œéµå­—ã€Œ{keyword}ã€
- è€ƒæ…®åŠ å…¥å¸å¼•é»æ“Šçš„å…ƒç´ ï¼ˆå¦‚ï¼šæ•¸å­—ã€å¹´ä»½ã€å¯¦ç”¨æ€§è©å½™ï¼‰

### 2. å…§å®¹æè¿°æœ€ä½³åŒ–
- å»ºè­°æè¿°é•·åº¦ï¼š{max(100, avg_snippet_length-20):.0f}-{avg_snippet_length+20:.0f} å­—å…ƒ
- ç•¶å‰å¹³å‡é•·åº¦ï¼š{avg_snippet_length:.0f} å­—å…ƒ
- åœ¨æè¿°ä¸­æ˜ç¢ºæåŠç›®æ¨™å—çœ¾ï¼š{audience}
- å¼·èª¿ç¨ç‰¹åƒ¹å€¼å’Œè§£æ±ºæ–¹æ¡ˆ

### 3. ç«¶çˆ­åˆ†ææ´å¯Ÿ
- å¸‚å ´ç«¶çˆ­ç¨‹åº¦ï¼š{"æ¿€çƒˆ" if total_results >= 10 else "ä¸­ç­‰"}
- ä¸»è¦ç«¶çˆ­é¡å‹ï¼š{"å•†æ¥­ç¶²ç«™" if any("shop" in result.link or "buy" in result.link for result in serp_result.organic_results) else "å…§å®¹å‹ç¶²ç«™"}

### 4. å…§å®¹ç­–ç•¥å»ºè­°
é‡å°ç›®æ¨™å—çœ¾ã€Œ{audience}ã€çš„å…§å®¹å»ºè­°ï¼š
- é‡é»çªå‡ºå¯¦ç”¨æ€§å’Œå¯æ“ä½œæ€§
- æä¾›å…·é«”çš„æ­¥é©ŸæŒ‡å—æˆ–å·¥å…·æ¨è–¦
- åŠ å…¥æ¡ˆä¾‹ç ”ç©¶æˆ–å¯¦éš›æ•ˆæœå±•ç¤º
- è€ƒæ…®è£½ä½œæ¯”è¼ƒè¡¨æ ¼æˆ–è©•æ¸¬å…§å®¹

## ğŸ“Š å¸‚å ´æ©Ÿæœƒåˆ†æ

åŸºæ–¼ SERP åˆ†æçš„å¸‚å ´æ©Ÿæœƒï¼š
1. **å…§å®¹ç©ºç™½é»è­˜åˆ¥**: åˆ†æç¾æœ‰å…§å®¹çš„å…±åŒå¼±é»
2. **å·®ç•°åŒ–æ©Ÿæœƒ**: å°‹æ‰¾èˆ‡çœ¾ä¸åŒçš„è§’åº¦åˆ‡å…¥
3. **ä½¿ç”¨è€…æ„åœ–æ»¿è¶³**: ç¢ºä¿å…§å®¹å®Œå…¨å°æ‡‰æœå°‹æ„åœ–

"""
    
    # æ ¹æ“šé¸é …æ·»åŠ é¡å¤–å…§å®¹
    if options.generate_draft:
        report += """
## ğŸ“ å…§å®¹å¤§ç¶±å»ºè­°

åŸºæ–¼ç«¶çˆ­å°æ‰‹åˆ†æï¼Œå»ºè­°çš„å…§å®¹çµæ§‹ï¼š

1. **å¼•è¨€éƒ¨åˆ†** (ç´„ 200-300 å­—)
   - é»å‡ºå•é¡Œç—›é»
   - æ‰¿è«¾è§£æ±ºæ–¹æ¡ˆåƒ¹å€¼

2. **ä¸»é«”å…§å®¹** (ç´„ 1500-2000 å­—)
   - è©³ç´°è§£ç­”æˆ–æŒ‡å—
   - å…·é«”æ­¥é©Ÿæˆ–å·¥å…·ä»‹ç´¹
   - å¯¦éš›æ¡ˆä¾‹åˆ†äº«

3. **ç¸½çµèˆ‡è¡Œå‹•å‘¼ç±²** (ç´„ 100-200 å­—)
   - é‡é»æ‘˜è¦
   - é¼“å‹µæ¡å–è¡Œå‹•
"""
    
    if options.include_faq:
        report += """
## â“ å»ºè­°å¸¸è¦‹å•é¡Œ

åŸºæ–¼æœå°‹çµæœåˆ†æï¼Œä½¿ç”¨è€…å¯èƒ½é—œå¿ƒçš„å•é¡Œï¼š
1. å¦‚ä½•é–‹å§‹ä½¿ç”¨ç›¸é—œå·¥å…·æˆ–æœå‹™ï¼Ÿ
2. è²»ç”¨å’Œæˆæœ¬è€ƒé‡æ˜¯ä»€éº¼ï¼Ÿ
3. æ•ˆæœå¤šä¹…èƒ½çœ‹åˆ°ï¼Ÿ
4. é©åˆå“ªäº›é¡å‹çš„ä¼æ¥­æˆ–å€‹äººï¼Ÿ
5. èˆ‡å…¶ä»–è§£æ±ºæ–¹æ¡ˆæœ‰ä½•å€åˆ¥ï¼Ÿ
"""
    
    if options.include_table:
        report += """
## ğŸ“‹ ç«¶çˆ­å°æ‰‹æ¯”è¼ƒè¡¨æ ¼

| æ’å | æ¨™é¡Œ | ç‰¹è‰² | ç›®æ¨™å—çœ¾ |
|------|------|------|----------|
"""
        for i, result in enumerate(serp_result.organic_results[:5], 1):
            title_short = result.title[:30] + "..." if len(result.title) > 30 else result.title
            report += f"| {i} | {title_short} | å¾…åˆ†æ | å¾…åˆ†æ |\n"
    
    report += f"""

---
*æ­¤å ±å‘Šç”± SEO Analyzer åŸºæ–¼çœŸå¯¦æœå°‹è³‡æ–™è‡ªå‹•ç”Ÿæˆ*  
*åˆ†æè³‡æ–™ä¾†æº: Google æœå°‹çµæœ (å…± {total_results} ç­†)*  
*ç”Ÿæˆæ™‚é–“: {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S')} UTC*"""
    
    return report


def create_error_response(error_code: str, message: str, details: Optional[dict] = None) -> ErrorResponse:
    """å»ºç«‹çµ±ä¸€æ ¼å¼çš„éŒ¯èª¤å›æ‡‰ã€‚

    Args:
        error_code: éŒ¯èª¤ä»£ç¢¼
        message: éŒ¯èª¤è¨Šæ¯
        details: éŒ¯èª¤è©³ç´°è³‡è¨Š

    Returns:
        ErrorResponse: æ¨™æº–æ ¼å¼çš„éŒ¯èª¤å›æ‡‰
    """
    error_detail = None
    if details:
        error_detail = ErrorDetail(
            field=details.get("field"),
            provided_value=details.get("provided_value"),
            expected_format=details.get("expected_format")
        )

    error_info = ErrorInfo(
        code=error_code,
        message=message,
        details=error_detail,
        timestamp=datetime.now(timezone.utc).isoformat()
    )

    return ErrorResponse(
        status="error",
        error=error_info
    )
