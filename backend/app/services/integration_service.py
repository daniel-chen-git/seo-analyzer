"""æ•´åˆæœå‹™æ¨¡çµ„ã€‚

æ­¤æ¨¡çµ„æä¾›å„å€‹æœå‹™é–“çš„è³‡æ–™è½‰æ›ã€æ•´åˆå”èª¿å’Œçµ±ä¸€éŒ¯èª¤è™•ç†åŠŸèƒ½ã€‚
è² è²¬å°‡ SERPã€çˆ¬èŸ²ã€AI æœå‹™æ•´åˆç‚ºå®Œæ•´çš„ SEO åˆ†ææµç¨‹ã€‚
"""

import time
import os
import json
import hashlib
from datetime import datetime, timezone
from typing import Dict, List, Optional

from .job_manager import JobManager

from ..models.request import AnalyzeRequest, AnalyzeOptions as RequestOptions
from ..models.response import (
    AnalyzeResponse, AnalysisData, SerpSummary, 
    AnalysisMetadata
)
from .serp_service import get_serp_service, SerpResult, SerpAPIException
from .scraper_service import get_scraper_service, ScrapingResult, ScraperException
from .ai_service import (
    get_ai_service, AnalysisOptions as AIOptions, AnalysisResult,
    AIServiceException, TokenLimitExceededException, AIAPIException
)


class IntegrationService:
    """æ•´åˆæœå‹™é¡åˆ¥ã€‚
    
    å”èª¿ SERPã€çˆ¬èŸ²ã€AI æœå‹™çš„å®Œæ•´åˆ†ææµç¨‹ï¼Œ
    æä¾›çµ±ä¸€çš„è³‡æ–™è½‰æ›å’ŒéŒ¯èª¤è™•ç†æ©Ÿåˆ¶ã€‚
    """

    def __init__(self):
        """åˆå§‹åŒ–æ•´åˆæœå‹™ã€‚"""
        self.serp_service = get_serp_service()
        self.scraper_service = get_scraper_service()
        self.ai_service = get_ai_service()
        
        # æ•ˆèƒ½ç›£æ§é…ç½®
        self.performance_thresholds = {
            "serp_duration": 15.0,      # SERP éšæ®µè­¦å‘Šé–¾å€¼
            "scraping_duration": 25.0,  # çˆ¬èŸ²éšæ®µè­¦å‘Šé–¾å€¼
            "ai_duration": 35.0,        # AI éšæ®µè­¦å‘Šé–¾å€¼
            "total_duration": 55.0      # ç¸½æ™‚é–“è­¦å‘Šé–¾å€¼
        }
        
        # å¿«å–è¨­å®š - ä½¿ç”¨ç•¶å‰æª”æ¡ˆçš„ç›¸å°è·¯å¾‘
        self.cache_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "cache")
    
    def _get_cache_file_path(self, keyword: str) -> str:
        """ç”ŸæˆåŸºæ–¼é—œéµå­— hash çš„å¿«å–æª”æ¡ˆè·¯å¾‘ã€‚
        
        Args:
            keyword: æœå°‹é—œéµå­—
            
        Returns:
            str: å¿«å–æª”æ¡ˆçš„å®Œæ•´è·¯å¾‘
        """
        # ç”Ÿæˆé—œéµå­—çš„ hash å€¼
        keyword_hash = hashlib.md5(keyword.encode('utf-8')).hexdigest()[:8]
        filename = f"analysis_result_{keyword_hash}.json"
        return os.path.join(self.cache_dir, filename)
    
    def _load_cached_result(self, keyword: str) -> Optional[AnalysisResult]:
        """å¾å¿«å–æª”æ¡ˆè¼‰å…¥åˆ†æçµæœã€‚
        
        æ”¯æ´å‘å¾Œç›¸å®¹ï¼šè‡ªå‹•ç‚ºèˆŠç‰ˆå¿«å–æª”æ¡ˆè£œå……ç¼ºå¤±çš„ status æ¬„ä½ã€‚
        
        Args:
            keyword: æœå°‹é—œéµå­—
            
        Returns:
            Optional[AnalysisResult]: å¿«å–çš„åˆ†æçµæœï¼Œå¦‚æœä¸å­˜åœ¨å‰‡è¿”å› None
        """
        cache_file = self._get_cache_file_path(keyword)
        
        if not os.path.exists(cache_file):
            print(f"ğŸ“‚ å¿«å–æª”æ¡ˆä¸å­˜åœ¨: {cache_file}")
            return None
        
        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
            
            # å‘å¾Œç›¸å®¹ï¼šç‚ºèˆŠç‰ˆå¿«å–æª”æ¡ˆè‡ªå‹•è£œå…… status æ¬„ä½
            if 'status' not in cache_data:
                cache_data['status'] = 'success'
                print(f"ğŸ”„ ç‚ºèˆŠç‰ˆå¿«å–æª”æ¡ˆè£œå…… status æ¬„ä½: {cache_file}")
                
                # æ›´æ–°å¿«å–æª”æ¡ˆä»¥åŒ…å« status æ¬„ä½
                try:
                    with open(cache_file, 'w', encoding='utf-8') as f:
                        json.dump(cache_data, f, ensure_ascii=False, indent=2)
                    print(f"ğŸ’¾ å·²æ›´æ–°å¿«å–æª”æ¡ˆæ ¼å¼: {cache_file}")
                except Exception as update_error:
                    print(f"âš ï¸ å¿«å–æª”æ¡ˆæ ¼å¼æ›´æ–°å¤±æ•—ï¼ˆä¸å½±éŸ¿è¼‰å…¥ï¼‰: {update_error}")
            
            print(f"ğŸ“‚ å¾å¿«å–è¼‰å…¥åˆ†æçµæœï¼ˆé›™æ¬„ä½æ ¼å¼ï¼‰: {cache_file}")
            
            # é‡å»º AnalysisResult ç‰©ä»¶ï¼ˆåªéœ€è¦æ¥­å‹™è³‡æ–™ï¼‰
            return AnalysisResult(
                analysis_report=cache_data['analysis_report'],
                token_usage=cache_data['token_usage'],
                processing_time=cache_data['processing_time'],
                success=cache_data['success']
            )
            
        except (json.JSONDecodeError, KeyError, FileNotFoundError) as e:
            print(f"âŒ å¿«å–æª”æ¡ˆè®€å–å¤±æ•—: {e}")
            return None
    
    def _save_result_to_cache(self, keyword: str, analysis_result: AnalysisResult) -> None:
        """å°‡åˆ†æçµæœå„²å­˜åˆ°å¿«å–æª”æ¡ˆï¼ˆé›™æ¬„ä½æ ¼å¼ï¼‰ã€‚
        
        å¿«å–æª”æ¡ˆæ¡ç”¨èˆ‡ AnalyzeResponse ä¸€è‡´çš„æ‰å¹³çµæ§‹ï¼ŒåŒ…å«é›™ç‹€æ…‹æ¬„ä½ï¼š
        - status: å›ºå®šç‚º "success"ï¼Œä¿æŒèˆ‡ API å›æ‡‰æ ¼å¼ä¸€è‡´
        - success: ä¾†è‡ª analysis_result.successï¼Œåæ˜ æ¥­å‹™è™•ç†çµæœ
        
        Args:
            keyword: æœå°‹é—œéµå­—
            analysis_result: è¦å„²å­˜çš„åˆ†æçµæœ
        """
        cache_file = self._get_cache_file_path(keyword)
        
        try:
            # æº–å‚™è¦å„²å­˜çš„è³‡æ–™ï¼ˆé›™æ¬„ä½æ ¼å¼ï¼Œèˆ‡ AnalyzeResponse ä¸€è‡´ï¼‰
            cache_data = {
                # API å¥‘ç´„æ¬„ä½ï¼šèˆ‡å›æ‡‰æ ¼å¼ä¿æŒä¸€è‡´
                'status': 'success',
                
                # æ ¸å¿ƒæ¥­å‹™è³‡æ–™
                'analysis_report': analysis_result.analysis_report,
                'token_usage': analysis_result.token_usage,
                'processing_time': analysis_result.processing_time,
                # æ¥­å‹™ç‹€æ…‹æ¬„ä½ï¼šåæ˜ å¯¦éš›è™•ç†çµæœ
                'success': analysis_result.success,
                'cached_at': datetime.now(timezone.utc).isoformat(),
                'keyword': keyword
            }
            
            # ç¢ºä¿ç›®éŒ„å­˜åœ¨
            os.makedirs(self.cache_dir, exist_ok=True)
            
            # å¯«å…¥æª”æ¡ˆ
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ åˆ†æçµæœå·²å„²å­˜åˆ°å¿«å–ï¼ˆé›™æ¬„ä½æ ¼å¼ï¼‰: {cache_file}")
            
        except Exception as e:
            print(f"âŒ å¿«å–æª”æ¡ˆå¯«å…¥å¤±æ•—: {e}")
            # ä¸æ‹‹å‡ºä¾‹å¤–ï¼Œè®“ä¸»æµç¨‹ç¹¼çºŒ
    
    def _load_cached_response(self, keyword: str) -> Optional[AnalyzeResponse]:
        """å¾å¿«å–æª”æ¡ˆç›´æ¥è¼‰å…¥ä¸¦è½‰æ›ç‚º AnalyzeResponseã€‚
        
        ç”¨æ–¼å¿«å–å‘½ä¸­çš„æƒ…æ³ï¼Œç›´æ¥è¿”å›å®Œæ•´çš„å›æ‡‰ç‰©ä»¶ã€‚
        è‡ªå‹•è™•ç†å‘å¾Œç›¸å®¹å’Œ status æ¬„ä½è£œå……ã€‚
        
        Args:
            keyword: æœå°‹é—œéµå­—
            
        Returns:
            Optional[AnalyzeResponse]: å®Œæ•´çš„å¿«å–å›æ‡‰ï¼Œå¦‚æœä¸å­˜åœ¨å‰‡è¿”å› None
        """
        cache_file = self._get_cache_file_path(keyword)
        
        if not os.path.exists(cache_file):
            return None
        
        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
            
            # å‘å¾Œç›¸å®¹ï¼šç‚ºèˆŠç‰ˆå¿«å–æª”æ¡ˆè‡ªå‹•è£œå…… status æ¬„ä½
            if 'status' not in cache_data:
                cache_data['status'] = 'success'
                print(f"ğŸ”„ å¿«å–å›æ‡‰è£œå…… status æ¬„ä½: {cache_file}")
                
                # æ›´æ–°å¿«å–æª”æ¡ˆ
                try:
                    with open(cache_file, 'w', encoding='utf-8') as f:
                        json.dump(cache_data, f, ensure_ascii=False, indent=2)
                except Exception:
                    pass  # éœé»˜è™•ç†æ›´æ–°å¤±æ•—
            
            # ç›´æ¥å»ºæ§‹ AnalyzeResponse ç‰©ä»¶ï¼ˆé›™æ¬„ä½æ ¼å¼ï¼‰
            return AnalyzeResponse(
                status=cache_data['status'],  # API å¥‘ç´„æ¬„ä½
                analysis_report=cache_data['analysis_report'],
                token_usage=cache_data['token_usage'],
                processing_time=cache_data['processing_time'],
                success=cache_data['success'],  # æ¥­å‹™ç‹€æ…‹æ¬„ä½
                cached_at=cache_data['cached_at'],
                keyword=cache_data['keyword']
            )
            
        except (json.JSONDecodeError, KeyError, FileNotFoundError) as e:
            print(f"âŒ å¿«å–å›æ‡‰è¼‰å…¥å¤±æ•—: {e}")
            return None
    
    async def execute_full_analysis(self, request: AnalyzeRequest) -> AnalyzeResponse:
        """åŸ·è¡Œå®Œæ•´çš„ SEO åˆ†ææµç¨‹ã€‚
        
        æ•´åˆ SERP æ“·å–ã€ç¶²é çˆ¬å–å’Œ AI åˆ†æï¼Œç”Ÿæˆå®Œæ•´çš„åˆ†æå ±å‘Šã€‚
        
        Args:
            request: SEO åˆ†æè«‹æ±‚
            
        Returns:
            AnalyzeResponse: å®Œæ•´çš„åˆ†æçµæœ
            
        Raises:
            å„ç¨®æœå‹™ç›¸é—œä¾‹å¤–
        """
        start_time = time.time()
        timer = PerformanceTimer()
        
        try:
            # éšæ®µ 1: SERP è³‡æ–™æ“·å–
            print(f"ğŸ” é–‹å§‹ SERP è³‡æ–™æ“·å–: {request.keyword}")
            timer.start_phase("serp")
            
            serp_data = await self.serp_service.search_keyword(
                keyword=request.keyword,
                num_results=10
            )
            
            timer.end_phase("serp")
            print(f"âœ… SERP æ“·å–å®Œæˆï¼Œå–å¾— {len(serp_data.organic_results)} å€‹çµæœ "
                  f"({timer.get_phase_duration('serp'):.2f}s)")
            
            # å°å‡º SERP è³‡æ–™å…§å®¹
            print("ğŸ“‹ SERP æ“·å–è³‡æ–™å…§å®¹ï¼š")
            for i, result in enumerate(serp_data.organic_results, 1):
                print(f"  {i}. {result.title[:100]}{'...' if len(result.title) > 100 else ''}")
                print(f"     URL: {result.link}")
                if result.snippet:
                    print(f"     æ‘˜è¦: {result.snippet[:200]}{'...' if len(result.snippet) > 200 else ''}")
                print()
            
            # éšæ®µ 2: ç¶²é å…§å®¹çˆ¬å–
            print("ğŸ•·ï¸ é–‹å§‹ç¶²é å…§å®¹çˆ¬å–")
            timer.start_phase("scraping")
            
            urls = self._extract_urls_from_serp(serp_data)
            scraping_data = await self.scraper_service.scrape_urls(urls)
            
            timer.end_phase("scraping")
            success_rate = scraping_data.successful_scrapes / scraping_data.total_results
            print(f"âœ… ç¶²é çˆ¬å–å®Œæˆï¼ŒæˆåŠŸç‡ {success_rate:.1%} "
                  f"({scraping_data.successful_scrapes}/{scraping_data.total_results}) "
                  f"({timer.get_phase_duration('scraping'):.2f}s)")
            
            # æª¢æŸ¥çˆ¬å–æˆåŠŸç‡
            if success_rate < 0.5:  # ä½æ–¼ 50% å‰‡è­¦å‘Š
                print(f"âš ï¸ çˆ¬å–æˆåŠŸç‡è¼ƒä½: {success_rate:.1%}")
            
            # éšæ®µ 3: AI åˆ†æå ±å‘Šç”Ÿæˆ
            print("ğŸ¤– é–‹å§‹ AI åˆ†æå ±å‘Šç”Ÿæˆ")
            timer.start_phase("ai")
            
            ai_options = self._convert_to_ai_options(request.options)
            analysis_result = await self.ai_service.analyze_seo_content(
                keyword=request.keyword,
                audience=request.audience,
                serp_data=serp_data,
                scraping_data=scraping_data,
                options=ai_options
            )
            
            timer.end_phase("ai")
            print(f"âœ… AI åˆ†æå®Œæˆï¼Œä½¿ç”¨ {analysis_result.token_usage} tokens "
                  f"({timer.get_phase_duration('ai'):.2f}s)")
            
            # éšæ®µ 4: çµæœæ•´åˆ
            total_time = time.time() - start_time
            print(f"ğŸ“‹ æ•´åˆåˆ†æçµæœï¼Œç¸½è€—æ™‚ {total_time:.2f}s")
            
            response = self._build_success_response(
                request=request,
                serp_data=serp_data,
                scraping_data=scraping_data,
                analysis_result=analysis_result,
                processing_time=total_time,
                timer=timer
            )
            
            # æ•ˆèƒ½è­¦å‘Šæª¢æŸ¥
            self._check_performance_warnings(timer)
            
            return response
            
        except Exception as e:
            processing_time = time.time() - start_time
            print(f"âŒ åˆ†ææµç¨‹å¤±æ•—: {str(e)} (è€—æ™‚ {processing_time:.2f}s)")
            raise e  # é‡æ–°æ‹‹å‡ºä¾‹å¤–ï¼Œç”±ä¸Šå±¤è™•ç†
    
    def _extract_urls_from_serp(self, serp_data: SerpResult) -> List[str]:
        """å¾ SERP è³‡æ–™ä¸­æå– URL æ¸…å–®ã€‚
        
        Args:
            serp_data: SERP æœå°‹çµæœ
            
        Returns:
            List[str]: URL æ¸…å–®
        """
        urls = []
        for result in serp_data.organic_results:
            if result.link and result.link.startswith(('http://', 'https://')):
                urls.append(result.link)
        
        print(f"ğŸ“ å¾ SERP æå– {len(urls)} å€‹æœ‰æ•ˆ URL")
        return urls
    
    def _convert_to_ai_options(self, request_options: RequestOptions) -> AIOptions:
        """è½‰æ›è«‹æ±‚é¸é …ç‚º AI æœå‹™é¸é …ã€‚
        
        Args:
            request_options: è«‹æ±‚ä¸­çš„é¸é …
            
        Returns:
            AIOptions: AI æœå‹™é¸é …
        """
        return AIOptions(
            generate_draft=request_options.generate_draft,
            include_faq=request_options.include_faq,
            include_table=request_options.include_table
        )
    
    def _build_success_response(
        self,
        request: AnalyzeRequest,
        serp_data: SerpResult,
        scraping_data: ScrapingResult,
        analysis_result: AnalysisResult,
        processing_time: float,
        timer: Optional['PerformanceTimer'] = None
    ) -> AnalyzeResponse:
        """å»ºç«‹æˆåŠŸå›æ‡‰ï¼ˆé›™æ¬„ä½è¨­è¨ˆï¼‰ã€‚
        
        é›™æ¬„ä½è¨­è¨ˆèªªæ˜ï¼š
        - status: å›ºå®šç‚º "success"ï¼Œç¶­è­· API å¥‘ç´„å’Œå‰ç«¯ç›¸å®¹æ€§
        - success: ä¾†è‡ª analysis_result.successï¼Œåæ˜ æ¥­å‹™å±¤å¯¦éš›è™•ç†çµæœ
        
        Args:
            request: åŸå§‹è«‹æ±‚
            serp_data: SERP è³‡æ–™
            scraping_data: çˆ¬èŸ²è³‡æ–™  
            analysis_result: AI åˆ†æçµæœï¼ˆåŒ…å«æ¥­å‹™æˆåŠŸç‹€æ…‹ï¼‰
            processing_time: ç¸½è™•ç†æ™‚é–“
            timer: æ•ˆèƒ½è¨ˆæ™‚å™¨ï¼ˆå¯é¸ï¼‰
            
        Returns:
            AnalyzeResponse: å®Œæ•´çš„æˆåŠŸå›æ‡‰ï¼ˆåŒ…å«é›™ç‹€æ…‹æ¬„ä½ï¼‰
        """
        # å»ºç«‹æ‰å¹³çµæ§‹å›æ‡‰ï¼ˆé›™æ¬„ä½è¨­è¨ˆï¼‰
        return AnalyzeResponse(
            # API å¥‘ç´„æ¬„ä½ï¼šå›ºå®šç‚º "success"ï¼Œç¶­è­·å‰ç«¯ response.status === "success" åˆ¤æ–·
            status="success",
            
            # æ ¸å¿ƒæ¥­å‹™è³‡æ–™
            analysis_report=analysis_result.analysis_report,
            token_usage=analysis_result.token_usage,
            processing_time=processing_time,
            # æ¥­å‹™ç‹€æ…‹æ¬„ä½ï¼šç›´æ¥åæ˜  AI æœå‹™å±¤çš„å¯¦éš›è™•ç†çµæœ
            success=analysis_result.success,
            cached_at=datetime.now(timezone.utc).isoformat(),
            keyword=request.keyword
        )
    
    
    def _check_performance_warnings(self, timer: 'PerformanceTimer') -> None:
        """æª¢æŸ¥æ•ˆèƒ½è­¦å‘Šã€‚
        
        Args:
            timer: æ•ˆèƒ½è¨ˆæ™‚å™¨
        """
        timings = timer.get_all_timings()
        
        for phase, duration in timings.items():
            if phase.endswith('_duration'):
                phase_name = phase.replace('_duration', '')
                threshold = self.performance_thresholds.get(phase, float('inf'))
                
                if duration > threshold:
                    print(f"âš ï¸ æ•ˆèƒ½è­¦å‘Š: {phase_name} éšæ®µè€—æ™‚ {duration:.2f}s "
                          f"(è¶…é {threshold}s é–¾å€¼)")

    async def execute_full_analysis_with_progress(
        self,
        request: AnalyzeRequest,
        job_manager: 'JobManager',
        job_id: str
    ) -> AnalyzeResponse:
        """åŸ·è¡Œå®Œæ•´åˆ†ææµç¨‹ä¸¦è¿½è¹¤é€²åº¦ã€‚

        æ­¤æ–¹æ³•èˆ‡ execute_full_analysis ç›¸åŒï¼Œä½†æœƒæ›´æ–°ä»»å‹™é€²åº¦ã€‚

        Args:
            request: SEO åˆ†æè«‹æ±‚
            job_manager: ä»»å‹™ç®¡ç†å™¨
            job_id: ä»»å‹™è­˜åˆ¥ç¢¼

        Returns:
            AnalyzeResponse: å®Œæ•´çš„åˆ†æçµæœ

        Raises:
            å„ç¨®æœå‹™ç›¸é—œä¾‹å¤–
        """
        start_time = time.time()
        timer = PerformanceTimer()

        try:
            # éšæ®µ 1: SERP è³‡æ–™æ“·å–
            job_manager.update_progress(
                job_id, 1, "æ­£åœ¨æ“·å– SERP è³‡æ–™...", 10.0
            )
            print(f"ğŸ” é–‹å§‹ SERP è³‡æ–™æ“·å–: {request.keyword}")
            timer.start_phase("serp")

            serp_data = await self.serp_service.search_keyword(
                keyword=request.keyword,
                num_results=10
            )

            timer.end_phase("serp")
            job_manager.update_progress(
                job_id, 1, "SERP è³‡æ–™æ“·å–å®Œæˆ", 30.0
            )
            print(f"âœ… SERP æ“·å–å®Œæˆï¼Œå–å¾— {len(serp_data.organic_results)} å€‹çµæœ "
                  f"({timer.get_phase_duration('serp'):.2f}s)")
            
            # å°å‡º SERP è³‡æ–™å…§å®¹
            print("ğŸ“‹ SERP æ“·å–è³‡æ–™å…§å®¹ï¼š")
            for i, result in enumerate(serp_data.organic_results, 1):
                print(f"  {i}. {result.title[:100]}{'...' if len(result.title) > 100 else ''}")
                print(f"     URL: {result.link}")
                if result.snippet:
                    print(f"     æ‘˜è¦: {result.snippet[:200]}{'...' if len(result.snippet) > 200 else ''}")
                print()

            # éšæ®µ 2: ç¶²é å…§å®¹çˆ¬å–
            job_manager.update_progress(
                job_id, 2, "æ­£åœ¨çˆ¬å–ç¶²é å…§å®¹...", 35.0
            )
            print("ğŸ•·ï¸ é–‹å§‹ç¶²é å…§å®¹çˆ¬å–")
            timer.start_phase("scraping")

            scraping_data = await self.scraper_service.scrape_urls(
                urls=[result.link for result in serp_data.organic_results]
            )

            timer.end_phase("scraping")
            job_manager.update_progress(
                job_id, 2, "ç¶²é çˆ¬å–å®Œæˆ", 60.0
            )
            print(f"âœ… çˆ¬å–å®Œæˆï¼ŒæˆåŠŸç‡ {scraping_data.successful_scrapes}/"
                  f"{scraping_data.total_results} "
                  f"({timer.get_phase_duration('scraping'):.2f}s)")

            # éšæ®µ 3: AI åˆ†æ
            job_manager.update_progress(
                job_id, 3, "æ­£åœ¨é€²è¡Œ AI åˆ†æ...", 65.0
            )
            print("ğŸ¤– é–‹å§‹ AI åˆ†æ")
            timer.start_phase("ai")

            ai_options = self._convert_to_ai_options(request.options)
            
            # å˜—è©¦å¾å¿«å–è¼‰å…¥åˆ†æçµæœ
            analysis_result = self._load_cached_result(request.keyword)
            
            if analysis_result is None:
                # å¿«å–ä¸å­˜åœ¨ï¼ŒåŸ·è¡Œå¯¦éš›çš„ AI åˆ†æ
                print("ğŸ¤– åŸ·è¡Œ AI åˆ†æï¼ˆæœªæ‰¾åˆ°å¿«å–ï¼‰")
                analysis_result = await self.ai_service.analyze_seo_content(
                    keyword=request.keyword,
                    audience=request.audience,
                    serp_data=serp_data,
                    scraping_data=scraping_data,
                    options=ai_options
                )
                
                # å°‡çµæœå„²å­˜åˆ°å¿«å–
                self._save_result_to_cache(request.keyword, analysis_result)
            else:
                # ä½¿ç”¨å¿«å–çš„çµæœ
                print("ğŸ“‚ ä½¿ç”¨å¿«å–çš„ AI åˆ†æçµæœ")

            timer.end_phase("ai")
            job_manager.update_progress(
                job_id, 3, "AI åˆ†æå®Œæˆ", 95.0
            )
            print(f"âœ… AI åˆ†æå®Œæˆï¼Œä½¿ç”¨ {analysis_result.token_usage} tokens "
                  f"({timer.get_phase_duration('ai'):.2f}s)")

            # å»ºæ§‹å›æ‡‰
            processing_time = time.time() - start_time
            response = self._build_success_response(
                request=request,
                serp_data=serp_data,
                scraping_data=scraping_data,
                analysis_result=analysis_result,
                processing_time=processing_time,
                timer=timer
            )

            # æª¢æŸ¥æ•ˆèƒ½è­¦å‘Š
            self._check_performance_warnings(timer)

            job_manager.update_progress(
                job_id, 3, "åˆ†æå®Œæˆ", 100.0
            )
            return response

        except Exception as e:
            # ä»»å‹™å¤±æ•—æ™‚æ›´æ–°ç‹€æ…‹
            job_manager.fail_job(job_id, str(e))
            raise


class PerformanceTimer:
    """æ•ˆèƒ½è¨ˆæ™‚å™¨ã€‚
    
    ç”¨æ–¼ç›£æ§å„å€‹è™•ç†éšæ®µçš„åŸ·è¡Œæ™‚é–“ã€‚
    """
    
    def __init__(self):
        """åˆå§‹åŒ–è¨ˆæ™‚å™¨ã€‚"""
        self.timings = {}
    
    def start_phase(self, phase_name: str) -> None:
        """é–‹å§‹è¨ˆæ™‚ç‰¹å®šéšæ®µã€‚
        
        Args:
            phase_name: éšæ®µåç¨±
        """
        self.timings[f"{phase_name}_start"] = time.time()
    
    def end_phase(self, phase_name: str) -> None:
        """çµæŸè¨ˆæ™‚ç‰¹å®šéšæ®µã€‚
        
        Args:
            phase_name: éšæ®µåç¨±
        """
        start_key = f"{phase_name}_start"
        duration_key = f"{phase_name}_duration"
        
        if start_key in self.timings:
            start_time = self.timings[start_key]
            self.timings[duration_key] = time.time() - start_time
    
    def get_phase_duration(self, phase_name: str) -> float:
        """å–å¾—ç‰¹å®šéšæ®µçš„æŒçºŒæ™‚é–“ã€‚
        
        Args:
            phase_name: éšæ®µåç¨±
            
        Returns:
            float: æŒçºŒæ™‚é–“ï¼ˆç§’ï¼‰
        """
        return self.timings.get(f"{phase_name}_duration", 0.0)
    
    def get_all_timings(self) -> Dict[str, float]:
        """å–å¾—æ‰€æœ‰è¨ˆæ™‚è³‡è¨Šã€‚
        
        Returns:
            Dict[str, float]: æ‰€æœ‰è¨ˆæ™‚è³‡è¨Š
        """
        return {k: v for k, v in self.timings.items() if k.endswith('_duration')}
    
    def get_summary(self) -> Dict[str, float]:
        """å–å¾—è¨ˆæ™‚æ‘˜è¦ã€‚
        
        Returns:
            Dict[str, float]: è¨ˆæ™‚æ‘˜è¦
        """
        duration_timings = self.get_all_timings()
        total_duration = sum(duration_timings.values())
        
        return {
            **duration_timings,
            "total_duration": total_duration
        }


# å…¨åŸŸæœå‹™å¯¦ä¾‹
_integration_service = None


def get_integration_service() -> IntegrationService:
    """å–å¾—æ•´åˆæœå‹™çš„å…¨åŸŸå¯¦ä¾‹ã€‚
    
    å¯¦ä½œå–®ä¾‹æ¨¡å¼ï¼Œç¢ºä¿æ•´å€‹æ‡‰ç”¨ç¨‹å¼ä½¿ç”¨åŒä¸€å€‹æœå‹™å¯¦ä¾‹ã€‚
    
    Returns:
        IntegrationService: æ•´åˆæœå‹™å¯¦ä¾‹
    """
    global _integration_service
    if _integration_service is None:
        _integration_service = IntegrationService()
    return _integration_service