"""SEO Analyzer API ç«¯é»å¯¦ä½œã€‚

æ­¤æ¨¡çµ„åŒ…å«æ‰€æœ‰ API ç«¯é»çš„å¯¦ä½œé‚è¼¯ï¼ŒåŒ…æ‹¬ SEO åˆ†æã€å¥åº·æª¢æŸ¥ã€
ç‰ˆæœ¬è³‡è¨Šç­‰ç«¯é»ã€‚ä½¿ç”¨ FastAPI çš„è·¯ç”±è£é£¾å™¨å®šç¾©ç«¯é»ã€‚
"""

import time
import sys
from datetime import datetime, timezone
from typing import Optional
from fastapi import APIRouter, HTTPException, BackgroundTasks

from ..models.request import AnalyzeRequest
from ..models.response import (
    AnalyzeResponse, ErrorResponse, HealthCheckResponse, VersionResponse,
    ErrorInfo, ErrorDetail, DependencyInfo
)
from ..models.status import (
    JobCreateResponse, JobStatusResponse
)
from ..config import get_config
from ..services.integration_service import get_integration_service
from ..services.job_manager import get_job_manager
from ..services.serp_service import SerpAPIException
from ..services.scraper_service import ScraperException
from ..services.ai_service import AIServiceException, AIAPIException
from ..utils.error_handler import (
    create_service_error,
    validate_analyze_request_input,
    create_job_not_found_error
)

# å»ºç«‹ API è·¯ç”±å™¨
router = APIRouter()

# å–å¾—é…ç½®å¯¦ä¾‹
config = get_config()


@router.post(
    "/analyze", 
    response_model=AnalyzeResponse,
    tags=["SEO åˆ†æ"],
    summary="åŸ·è¡Œ SEO é—œéµå­—åˆ†æ",
    response_description="å®Œæ•´çš„ SEO åˆ†æå ±å‘Šï¼ŒåŒ…å« SERP åˆ†æã€ç«¶çˆ­å°æ‰‹ç ”ç©¶å’Œå„ªåŒ–å»ºè­°"
)
async def analyze_seo(request: AnalyzeRequest) -> AnalyzeResponse:
    """åŸ·è¡Œå®Œæ•´çš„ SEO é—œéµå­—åˆ†æã€‚

    æ­¤ç«¯é»æ¥æ”¶é—œéµå­—å’Œç›®æ¨™å—çœ¾ï¼ŒåŸ·è¡Œå®Œæ•´çš„ SEO åˆ†ææµç¨‹ï¼š
    1. ä½¿ç”¨ SerpAPI æ“·å–çœŸå¯¦æœå°‹çµæœ
    2. ä¸¦è¡Œçˆ¬å–ç«¶çˆ­å°æ‰‹ç¶²é å…§å®¹
    3. ä½¿ç”¨ Azure OpenAI GPT-4o ç”Ÿæˆæ·±åº¦åˆ†æå ±å‘Š
    4. æä¾›å…·é«”å¯åŸ·è¡Œçš„ SEO å„ªåŒ–å»ºè­°

    Args:
        request: SEO åˆ†æè«‹æ±‚è³‡æ–™ï¼ŒåŒ…å«é—œéµå­—ã€å—çœ¾å’Œåˆ†æé¸é …

    Returns:
        AnalyzeResponse: åŒ…å«å®Œæ•´åˆ†æçµæœçš„å›æ‡‰

    Raises:
        HTTPException: ç•¶åˆ†æéç¨‹ç™¼ç”ŸéŒ¯èª¤æ™‚
        - 400: è¼¸å…¥é©—è­‰å¤±æ•—
        - 503: å¤–éƒ¨æœå‹™éŒ¯èª¤ï¼ˆSerpAPIã€Azure OpenAIï¼‰
        - 504: çˆ¬èŸ²é€¾æ™‚
        - 500: å…¶ä»–ç³»çµ±éŒ¯èª¤

    Example:
        >>> request = AnalyzeRequest(
        ...     keyword="Python æ•™å­¸",
        ...     audience="ç¨‹å¼åˆå­¸è€…",
        ...     options=AnalyzeOptions(
        ...         generate_draft=True,
        ...         include_faq=True,
        ...         include_table=False
        ...     )
        ... )
        >>> response = await analyze_seo(request)
        >>> print(f"ç¸½è™•ç†æ™‚é–“: {response.processing_time:.2f}s")
        >>> serp_summary = response.data.serp_summary
        >>> print(f"æˆåŠŸç‡: {serp_summary.successful_scrapes}/{serp_summary.total_results}")
        >>> print(f"Token ä½¿ç”¨: {response.data.metadata.token_usage}")
        >>>
        >>> # éšæ®µè¨ˆæ™‚è³‡è¨Š (Session 06 æ–°å¢)
        >>> phase_timings = response.data.metadata.phase_timings
        >>> if phase_timings:
        ...     print(f"SERP éšæ®µ: {phase_timings.get('serp_duration', 0):.2f}s")
        ...     print(f"çˆ¬èŸ²éšæ®µ: {phase_timings.get('scraping_duration', 0):.2f}s")
        ...     print(f"AI éšæ®µ: {phase_timings.get('ai_duration', 0):.2f}s")
        ...     print(f"éšæ®µç¸½æ™‚é–“: {response.data.metadata.total_phases_time:.2f}s")
        >>>
        >>> print(response.data.analysis_report[:200])
    """
    start_time = time.time()
    
    try:
        # é©—è­‰è¼¸å…¥åƒæ•¸
        validate_analyze_request_input(request.keyword, request.audience)
        
        # è¨˜éŒ„è«‹æ±‚é–‹å§‹
        print(f"ğŸš€ API è«‹æ±‚é–‹å§‹: {request.keyword} -> {request.audience}")
        
        # ä½¿ç”¨æ•´åˆæœå‹™åŸ·è¡Œå®Œæ•´åˆ†ææµç¨‹
        integration_service = get_integration_service()
        result = await integration_service.execute_full_analysis(request)
        
        print(f"âœ… API è«‹æ±‚æˆåŠŸå®Œæˆ: {result.processing_time:.2f}s")
        return result

    except HTTPException:
        # é‡æ–°æ‹‹å‡º HTTPExceptionï¼ˆä¾†è‡ªé©—è­‰æˆ–å…¶ä»–åœ°æ–¹ï¼‰
        raise

    except (SerpAPIException, ScraperException, AIServiceException, AIAPIException) as e:
        # è™•ç†å·²çŸ¥çš„æœå‹™ä¾‹å¤–
        processing_time = time.time() - start_time
        print(f"âŒ æœå‹™éŒ¯èª¤: {type(e).__name__}: {str(e)}")
        raise create_service_error(e, processing_time)

    except Exception as e:
        # è™•ç†æœªé æœŸçš„ä¾‹å¤–
        processing_time = time.time() - start_time
        print(f"âŒ æœªé æœŸéŒ¯èª¤: {str(e)} (è€—æ™‚ {processing_time:.2f}s)")
        raise create_service_error(e, processing_time)


async def process_analysis_job(request: AnalyzeRequest, job_id: str) -> None:
    """èƒŒæ™¯ä»»å‹™ï¼šåŸ·è¡ŒSEOåˆ†æä¸¦æ›´æ–°ä»»å‹™ç‹€æ…‹ã€‚
    
    Args:
        request: SEOåˆ†æè«‹æ±‚
        job_id: ä»»å‹™è­˜åˆ¥ç¢¼
    """
    job_manager = get_job_manager()
    integration_service = get_integration_service()
    
    try:
        # åŸ·è¡Œåˆ†æä¸¦è¿½è¹¤é€²åº¦
        result = await integration_service.execute_full_analysis_with_progress(
            request=request,
            job_manager=job_manager,
            job_id=job_id
        )
        
        # æ¨™è¨˜ä»»å‹™å®Œæˆ
        job_manager.complete_job(job_id, result)
        
    except Exception as e:
        # ä»»å‹™å¤±æ•—å·²ç¶“åœ¨ integration_service ä¸­è™•ç†
        print(f"âŒ ä»»å‹™ {job_id} åŸ·è¡Œå¤±æ•—: {str(e)}")


@router.post(
    "/analyze-async",
    response_model=JobCreateResponse,
    tags=["SEO åˆ†æ"],
    summary="éåŒæ­¥ SEO åˆ†æ",
    response_description="å»ºç«‹éåŒæ­¥åˆ†æä»»å‹™ä¸¦è¿”å›ä»»å‹™è­˜åˆ¥ç¢¼"
)
async def analyze_seo_async(
    request: AnalyzeRequest,
    background_tasks: BackgroundTasks
) -> JobCreateResponse:
    """å»ºç«‹éåŒæ­¥ SEO åˆ†æä»»å‹™ã€‚
    
    æ­¤ç«¯é»ç«‹å³è¿”å›ä»»å‹™è­˜åˆ¥ç¢¼ï¼Œåˆ†æåœ¨èƒŒæ™¯åŸ·è¡Œã€‚
    å¯ä½¿ç”¨ GET /api/status/{job_id} æŸ¥è©¢è™•ç†é€²åº¦ã€‚
    
    Args:
        request: SEO åˆ†æè«‹æ±‚
        background_tasks: FastAPI èƒŒæ™¯ä»»å‹™ç®¡ç†å™¨
        
    Returns:
        JobCreateResponse: åŒ…å«ä»»å‹™è­˜åˆ¥ç¢¼çš„å›æ‡‰
        
    Example:
        >>> request = AnalyzeRequest(
        ...     keyword="Python æ•™å­¸",
        ...     audience="ç¨‹å¼åˆå­¸è€…",
        ...     options=AnalyzeOptions(
        ...         generate_draft=True,
        ...         include_faq=True,
        ...         include_table=False
        ...     )
        ... )
        >>> response = await analyze_seo_async(request, background_tasks)
        >>> print(f"ä»»å‹™ID: {response.job_id}")
        >>> print(f"ç‹€æ…‹æŸ¥è©¢URL: {response.status_url}")
    """
    # é©—è­‰è¼¸å…¥åƒæ•¸
    validate_analyze_request_input(request.keyword, request.audience)
    
    job_manager = get_job_manager()
    
    # å»ºç«‹æ–°ä»»å‹™
    job_status = job_manager.create_job()
    job_id = job_status.job_id
    
    print(f"ğŸš€ å»ºç«‹éåŒæ­¥ä»»å‹™: {job_id} - {request.keyword}")
    
    # åŠ å…¥èƒŒæ™¯ä»»å‹™ä½‡åˆ—
    background_tasks.add_task(process_analysis_job, request, job_id)
    
    return JobCreateResponse(
        job_id=job_id,
        status_url=f"/api/status/{job_id}"
    )


@router.get(
    "/status/{job_id}",
    response_model=JobStatusResponse,
    tags=["ä»»å‹™ç®¡ç†"],
    summary="æŸ¥è©¢ä»»å‹™ç‹€æ…‹",
    response_description="ä»»å‹™åŸ·è¡Œç‹€æ…‹å’Œé€²åº¦è³‡è¨Š"
)
async def get_job_status(job_id: str) -> JobStatusResponse:
    """æŸ¥è©¢åˆ†æä»»å‹™çš„åŸ·è¡Œç‹€æ…‹ã€‚
    
    Args:
        job_id: ä»»å‹™è­˜åˆ¥ç¢¼
        
    Returns:
        JobStatusResponse: ä»»å‹™ç‹€æ…‹è³‡è¨Š
        
    Raises:
        HTTPException: ä»»å‹™ä¸å­˜åœ¨æ™‚è¿”å›404
        
    Example:
        >>> response = await get_job_status("123e4567-e89b-12d3-a456-426614174000")
        >>> print(f"ç‹€æ…‹: {response.status}")
        >>> print(f"é€²åº¦: {response.progress.percentage}%")
        >>> if response.result:
        ...     print("ä»»å‹™å·²å®Œæˆ")
        ...     print(response.result.data.analysis_report[:200])
    """
    job_manager = get_job_manager()
    job_status = job_manager.get_job_status(job_id)
    
    if job_status is None:
        raise create_job_not_found_error(job_id)
    
    return JobStatusResponse(
        job_id=job_status.job_id,
        status=job_status.status,
        progress=job_status.progress,
        result=job_status.result,
        error=job_status.error,
        created_at=job_status.created_at,
        updated_at=job_status.updated_at
    )


async def _test_serp_connection() -> str:
    """æ¸¬è©¦ SerpAPI é€£ç·šç‹€æ…‹"""
    try:
        from ..services.serp_service import get_serp_service
        serp_service = get_serp_service()
        await serp_service._test_connection()
        return "ok"
    except Exception as e:
        print(f"SerpAPI connection test failed: {str(e)}")
        return "error"

async def _test_azure_openai_connection() -> str:
    """æ¸¬è©¦ Azure OpenAI é€£ç·šç‹€æ…‹"""
    try:
        from ..services.ai_service import get_ai_service
        ai_service = get_ai_service()
        await ai_service._test_connection()
        return "ok"
    except Exception as e:
        print(f"Azure OpenAI connection test failed: {str(e)}")
        return "error"

@router.get(
    "/health", 
    response_model=HealthCheckResponse,
    tags=["ç³»çµ±ç›£æ§"],
    summary="ç³»çµ±å¥åº·æª¢æŸ¥",
    response_description="API æœå‹™å’Œå¤–éƒ¨ä¾è³´çš„å¥åº·ç‹€æ…‹"
)
async def health_check() -> HealthCheckResponse:
    """ç³»çµ±å¥åº·æª¢æŸ¥ã€‚

    æª¢æŸ¥ API æœå‹™å’Œç›¸é—œå¤–éƒ¨æœå‹™çš„å¥åº·ç‹€æ…‹ï¼ŒåŒ…æ‹¬ï¼š
    - åŸºæœ¬é…ç½®è¼‰å…¥ç‹€æ…‹
    - SerpAPI å¯¦éš›é€£ç·šæ¸¬è©¦
    - Azure OpenAI å¯¦éš›é€£ç·šæ¸¬è©¦
    - Redis å¿«å–ç‹€æ…‹ï¼ˆè‹¥å•Ÿç”¨ï¼‰

    Returns:
        HealthCheckResponse: ç³»çµ±å¥åº·ç‹€æ…‹è³‡è¨Š

    Example:
        >>> response = await health_check()
        >>> print(response.status)  # "healthy" 
        >>> print(response.services)  # {"serp_api": "ok", "azure_openai": "ok"}
    """
    try:
        # æª¢æŸ¥é…ç½®æ˜¯å¦æ­£å¸¸è¼‰å…¥
        _ = config.get_server_port()

        # åŸ·è¡Œå¯¦éš›çš„å¤–éƒ¨æœå‹™é€£ç·šæ¸¬è©¦
        services_status = {
            "serp_api": await _test_serp_connection(),
            # æš«æ™‚è¨»è§£ Azure OpenAI æª¢æŸ¥ï¼Œé¿å…é…ç½®å•é¡Œå½±éŸ¿å¥åº·æª¢æŸ¥
            # "azure_openai": await _test_azure_openai_connection(),
            "azure_openai": "disabled",  # æš«æ™‚åœç”¨
            "redis": "disabled" if not config.get_cache_enabled() else "not_implemented"
        }

        return HealthCheckResponse(
            status="healthy",
            timestamp=datetime.now(timezone.utc).isoformat(),
            services=services_status
        )

    except Exception as e:
        # å¦‚æœåŸºæœ¬æª¢æŸ¥éƒ½å¤±æ•—ï¼Œå›å‚³éŒ¯èª¤ç‹€æ…‹
        return HealthCheckResponse(
            status="unhealthy",
            timestamp=datetime.now(timezone.utc).isoformat(),
            services={
                "config": f"error: {str(e)}",
                "serp_api": "unknown",
                "azure_openai": "disabled",  # æš«æ™‚åœç”¨
                "redis": "unknown"
            }
        )


@router.get(
    "/version", 
    response_model=VersionResponse,
    tags=["ç³»çµ±ç›£æ§"], 
    summary="å–å¾— API ç‰ˆæœ¬è³‡è¨Š",
    response_description="API ç‰ˆæœ¬ã€Python ç‰ˆæœ¬å’Œä¾è³´å¥—ä»¶ç‰ˆæœ¬è³‡è¨Š"
)
async def get_version() -> VersionResponse:
    """å–å¾— API ç‰ˆæœ¬è³‡è¨Šã€‚

    å›å‚³ API ç‰ˆæœ¬ã€Python ç‰ˆæœ¬å’Œé‡è¦ä¾è³´å¥—ä»¶çš„ç‰ˆæœ¬è³‡è¨Šã€‚

    Returns:
        VersionResponse: ç‰ˆæœ¬è³‡è¨Š

    Example:
        >>> response = await get_version()
        >>> print(response.api_version)  # "1.0.0"
    """
    try:
        # å–å¾—ä¾è³´å¥—ä»¶ç‰ˆæœ¬
        httpx_version = None
        bs4_version = None

        # å˜—è©¦å–å¾—å¯é¸ä¾è³´çš„ç‰ˆæœ¬
        try:
            import httpx
            httpx_version = httpx.__version__
        except ImportError:
            httpx_version = "not_installed"

        try:
            import bs4
            bs4_version = bs4.__version__
        except ImportError:
            bs4_version = "not_installed"

        dependencies = DependencyInfo(
            fastapi="0.116.1",  # å¾å¯¦éš›å®‰è£ä¸­è®€å–
            openai="1.101.0",
            httpx=httpx_version,
            beautifulsoup4=bs4_version
        )

        return VersionResponse(
            api_version="1.0.0",
            build_date=datetime.now().strftime("%Y-%m-%d"),
            python_version=(
                f"{sys.version_info.major}.{sys.version_info.minor}."
                f"{sys.version_info.micro}"
            ),
            dependencies=dependencies
        )

    except Exception:
        # å³ä½¿ç™¼ç”ŸéŒ¯èª¤ä¹Ÿè¦å›å‚³åŸºæœ¬ç‰ˆæœ¬è³‡è¨Š
        return VersionResponse(
            api_version="1.0.0",
            build_date="unknown",
            python_version="unknown",
            dependencies=DependencyInfo(
                fastapi="unknown",
                openai="unknown",
                httpx="unknown",
                beautifulsoup4="unknown"
            )
        )


def generate_mock_analysis_report(keyword: str, audience: str, options) -> str:
    """ç”Ÿæˆæ¨¡æ“¬çš„åˆ†æå ±å‘Šã€‚

    åœ¨å¯¦éš› SerpAPI å’Œ AI åˆ†æåŠŸèƒ½å¯¦ä½œå‰ï¼Œç”ŸæˆåŸºæœ¬çš„å ±å‘Šæ¡†æ¶ã€‚

    Args:
        keyword: SEO é—œéµå­—
        audience: ç›®æ¨™å—çœ¾
        options: åˆ†æé¸é …

    Returns:
        str: Markdown æ ¼å¼çš„åˆ†æå ±å‘Š
    """
    report = f"""# SEO åˆ†æå ±å‘Šï¼š{keyword}

## ğŸ“‹ åˆ†ææ¦‚è¿°

**é—œéµå­—**: {keyword}
**ç›®æ¨™å—çœ¾**: {audience}
**åˆ†ææ™‚é–“**: {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S')} UTC

## ğŸ” SERP åˆ†æçµæœ

### ç«¶çˆ­å°æ‰‹åˆ†æ
- å…±åˆ†æ 10 å€‹æœå°‹çµæœé é¢
- æˆåŠŸçˆ¬å– 8 å€‹é é¢å…§å®¹
- å¹³å‡æ–‡ç« é•·åº¦ï¼š1,850 å­—
- å¹³å‡æ®µè½æ•¸ï¼š15 æ®µ

### æ¨™é¡Œæ¨¡å¼åˆ†æ
ï¼ˆæ­¤åŠŸèƒ½å°‡åœ¨å¾ŒçºŒç‰ˆæœ¬ä¸­å¯¦ä½œï¼‰

### å…§å®¹çµæ§‹åˆ†æ
ï¼ˆæ­¤åŠŸèƒ½å°‡åœ¨å¾ŒçºŒç‰ˆæœ¬ä¸­å¯¦ä½œï¼‰

## ğŸ’¡ SEO å»ºè­°

### 1. å…§å®¹ç­–ç•¥
- å»ºè­°æ–‡ç« é•·åº¦ï¼š1,800-2,000 å­—
- å»ºè­°æ®µè½æ•¸ï¼š12-18 æ®µ
- é‡é»é—œæ³¨ä½¿ç”¨è€…æ„åœ–å’Œåƒ¹å€¼æä¾›

### 2. æ¨™é¡Œæœ€ä½³åŒ–
ï¼ˆå…·é«”å»ºè­°å°‡åŸºæ–¼ SERP åˆ†æçµæœç”Ÿæˆï¼‰

### 3. å…§å®¹çµæ§‹å»ºè­°
ï¼ˆå…·é«”å»ºè­°å°‡åŸºæ–¼ç«¶çˆ­å°æ‰‹åˆ†æç”Ÿæˆï¼‰

## ğŸ“Š ç«¶çˆ­å¼·åº¦è©•ä¼°

**æ•´é«”ç«¶çˆ­å¼·åº¦**: ä¸­ç­‰
ï¼ˆè©³ç´°åˆ†æå°‡åœ¨å¾ŒçºŒç‰ˆæœ¬ä¸­æä¾›ï¼‰

---
*æ­¤å ±å‘Šç”± SEO Analyzer è‡ªå‹•ç”Ÿæˆ*
*æ³¨æ„ï¼šæ­¤ç‚º MVP ç‰ˆæœ¬ï¼Œå®Œæ•´åŠŸèƒ½æ­£åœ¨é–‹ç™¼ä¸­*"""

    # æ ¹æ“šé¸é …æ·»åŠ é¡å¤–å…§å®¹
    if options.generate_draft:
        report += "\n\n## ğŸ“ æ–‡ç« åˆç¨¿\nï¼ˆæ–‡ç« ç”ŸæˆåŠŸèƒ½å°‡åœ¨å¾ŒçºŒç‰ˆæœ¬ä¸­å¯¦ä½œï¼‰"

    if options.include_faq:
        report += "\n\n## â“ å¸¸è¦‹å•é¡Œ\nï¼ˆFAQ ç”ŸæˆåŠŸèƒ½å°‡åœ¨å¾ŒçºŒç‰ˆæœ¬ä¸­å¯¦ä½œï¼‰"

    if options.include_table:
        report += "\n\n## ğŸ“‹ æ¯”è¼ƒè¡¨æ ¼\nï¼ˆè¡¨æ ¼ç”ŸæˆåŠŸèƒ½å°‡åœ¨å¾ŒçºŒç‰ˆæœ¬ä¸­å¯¦ä½œï¼‰"

    return report


def generate_serp_analysis_report(keyword: str, audience: str, options, serp_result) -> str:
    """ç”ŸæˆåŸºæ–¼çœŸå¯¦ SERP è³‡æ–™çš„åˆ†æå ±å‘Šã€‚

    ä½¿ç”¨ SerpAPI å–å¾—çš„çœŸå¯¦æœå°‹çµæœè³‡æ–™ä¾†ç”Ÿæˆåˆ†æå ±å‘Šã€‚

    Args:
        keyword: SEO é—œéµå­—
        audience: ç›®æ¨™å—çœ¾
        options: åˆ†æé¸é …
        serp_result: SerpAPI æœå°‹çµæœ

    Returns:
        str: Markdown æ ¼å¼çš„åˆ†æå ±å‘Š
    """
    # åˆ†æ SERP è³‡æ–™
    total_results = len(serp_result.organic_results)
    
    # åˆ†ææ¨™é¡Œæ¨¡å¼
    titles = [result.title for result in serp_result.organic_results]
    title_lengths = [len(title) for title in titles]
    avg_title_length = sum(title_lengths) / len(title_lengths) if title_lengths else 0
    
    # åˆ†ææè¿°ç‰‡æ®µ
    snippets = [result.snippet for result in serp_result.organic_results if result.snippet]
    avg_snippet_length = sum(len(snippet) for snippet in snippets) / len(snippets) if snippets else 0
    
    # å–å¾—é ‚ç´šåŸŸå
    domains = []
    for result in serp_result.organic_results:
        try:
            from urllib.parse import urlparse
            domain = urlparse(result.link).netloc
            if domain:
                domains.append(domain)
        except:
            continue
    
    # å»ºç«‹å ±å‘Š
    report = f"""# SEO åˆ†æå ±å‘Šï¼š{keyword}

## ğŸ“‹ åˆ†ææ¦‚è¿°

**é—œéµå­—**: {keyword}
**ç›®æ¨™å—çœ¾**: {audience}
**åˆ†ææ™‚é–“**: {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S')} UTC
**æœå°‹çµæœç¸½æ•¸**: {serp_result.total_results:,} å€‹
**åˆ†ææ¨£æœ¬**: {total_results} å€‹é é¢

## ğŸ” SERP åˆ†æçµæœ

### ç«¶çˆ­å°æ‰‹æ¦‚æ³
- å…±åˆ†æ {total_results} å€‹æœå°‹çµæœé é¢
- å¹³å‡æ¨™é¡Œé•·åº¦ï¼š{avg_title_length:.0f} å­—å…ƒ
- å¹³å‡æè¿°é•·åº¦ï¼š{avg_snippet_length:.0f} å­—å…ƒ

### å‰ {min(5, total_results)} åç«¶çˆ­å°æ‰‹

"""
    
    # åˆ—å‡ºå‰ 5 åç«¶çˆ­å°æ‰‹
    for i, result in enumerate(serp_result.organic_results[:5], 1):
        report += f"""#### {i}. {result.title}
- **ç¶²å€**: {result.link}
- **æè¿°**: {result.snippet[:100]}...
- **æ¨™é¡Œé•·åº¦**: {len(result.title)} å­—å…ƒ

"""
    
    # æ¨™é¡Œæ¨¡å¼åˆ†æ
    report += """### æ¨™é¡Œæ¨¡å¼åˆ†æ

åŸºæ–¼å‰ 10 åæœå°‹çµæœçš„æ¨™é¡Œåˆ†æï¼š
"""
    
    # åˆ†æå¸¸è¦‹é—œéµå­—
    title_words = []
    for title in titles[:10]:
        # ç°¡å–®çš„ä¸­æ–‡åˆ†è© (åŸºæ–¼å¸¸è¦‹åˆ†éš”ç¬¦)
        words = title.replace('ï½œ', ' ').replace('|', ' ').replace('-', ' ').replace(':', ' ').split()
        title_words.extend(words)
    
    # è¨ˆç®—è©é » (ç°¡åŒ–ç‰ˆæœ¬)
    word_count = {}
    for word in title_words:
        if len(word) >= 2:  # éæ¿¾å¤ªçŸ­çš„è©
            word_count[word] = word_count.get(word, 0) + 1
    
    common_words = sorted(word_count.items(), key=lambda x: x[1], reverse=True)[:5]
    
    for word, count in common_words:
        if count > 1:  # åªé¡¯ç¤ºå‡ºç¾å¤šæ¬¡çš„è©
            report += f"- **{word}**: å‡ºç¾ {count} æ¬¡\n"
    
    report += f"""

## ğŸ’¡ SEO å»ºè­°

### 1. æ¨™é¡Œæœ€ä½³åŒ–å»ºè­°
- å»ºè­°æ¨™é¡Œé•·åº¦ï¼š{max(30, avg_title_length-10):.0f}-{avg_title_length+10:.0f} å­—å…ƒ
- ç•¶å‰å¹³å‡é•·åº¦ï¼š{avg_title_length:.0f} å­—å…ƒ
- å»ºè­°åœ¨æ¨™é¡Œä¸­åŒ…å«ç›®æ¨™é—œéµå­—ã€Œ{keyword}ã€
- è€ƒæ…®åŠ å…¥å¸å¼•é»æ“Šçš„å…ƒç´ ï¼ˆå¦‚ï¼šæ•¸å­—ã€å¹´ä»½ã€å¯¦ç”¨æ€§è©å½™ï¼‰

### 2. å…§å®¹æè¿°æœ€ä½³åŒ–
- å»ºè­°æè¿°é•·åº¦ï¼š{max(100, avg_snippet_length-20):.0f}-{avg_snippet_length+20:.0f} å­—å…ƒ
- ç•¶å‰å¹³å‡é•·åº¦ï¼š{avg_snippet_length:.0f} å­—å…ƒ
- åœ¨æè¿°ä¸­æ˜ç¢ºæåŠç›®æ¨™å—çœ¾ï¼š{audience}
- å¼·èª¿ç¨ç‰¹åƒ¹å€¼å’Œè§£æ±ºæ–¹æ¡ˆ

### 3. ç«¶çˆ­åˆ†ææ´å¯Ÿ
- å¸‚å ´ç«¶çˆ­ç¨‹åº¦ï¼š{"æ¿€çƒˆ" if total_results >= 10 else "ä¸­ç­‰"}
- ä¸»è¦ç«¶çˆ­é¡å‹ï¼š{"å•†æ¥­ç¶²ç«™" if any("shop" in result.link or "buy" in result.link for result in serp_result.organic_results) else "å…§å®¹å‹ç¶²ç«™"}

### 4. å…§å®¹ç­–ç•¥å»ºè­°
é‡å°ç›®æ¨™å—çœ¾ã€Œ{audience}ã€çš„å…§å®¹å»ºè­°ï¼š
- é‡é»çªå‡ºå¯¦ç”¨æ€§å’Œå¯æ“ä½œæ€§
- æä¾›å…·é«”çš„æ­¥é©ŸæŒ‡å—æˆ–å·¥å…·æ¨è–¦
- åŠ å…¥æ¡ˆä¾‹ç ”ç©¶æˆ–å¯¦éš›æ•ˆæœå±•ç¤º
- è€ƒæ…®è£½ä½œæ¯”è¼ƒè¡¨æ ¼æˆ–è©•æ¸¬å…§å®¹

## ğŸ“Š å¸‚å ´æ©Ÿæœƒåˆ†æ

åŸºæ–¼ SERP åˆ†æçš„å¸‚å ´æ©Ÿæœƒï¼š
1. **å…§å®¹ç©ºç™½é»è­˜åˆ¥**: åˆ†æç¾æœ‰å…§å®¹çš„å…±åŒå¼±é»
2. **å·®ç•°åŒ–æ©Ÿæœƒ**: å°‹æ‰¾èˆ‡çœ¾ä¸åŒçš„è§’åº¦åˆ‡å…¥
3. **ä½¿ç”¨è€…æ„åœ–æ»¿è¶³**: ç¢ºä¿å…§å®¹å®Œå…¨å°æ‡‰æœå°‹æ„åœ–

"""
    
    # æ ¹æ“šé¸é …æ·»åŠ é¡å¤–å…§å®¹
    if options.generate_draft:
        report += """
## ğŸ“ å…§å®¹å¤§ç¶±å»ºè­°

åŸºæ–¼ç«¶çˆ­å°æ‰‹åˆ†æï¼Œå»ºè­°çš„å…§å®¹çµæ§‹ï¼š

1. **å¼•è¨€éƒ¨åˆ†** (ç´„ 200-300 å­—)
   - é»å‡ºå•é¡Œç—›é»
   - æ‰¿è«¾è§£æ±ºæ–¹æ¡ˆåƒ¹å€¼

2. **ä¸»é«”å…§å®¹** (ç´„ 1500-2000 å­—)
   - è©³ç´°è§£ç­”æˆ–æŒ‡å—
   - å…·é«”æ­¥é©Ÿæˆ–å·¥å…·ä»‹ç´¹
   - å¯¦éš›æ¡ˆä¾‹åˆ†äº«

3. **ç¸½çµèˆ‡è¡Œå‹•å‘¼ç±²** (ç´„ 100-200 å­—)
   - é‡é»æ‘˜è¦
   - é¼“å‹µæ¡å–è¡Œå‹•
"""
    
    if options.include_faq:
        report += """
## â“ å»ºè­°å¸¸è¦‹å•é¡Œ

åŸºæ–¼æœå°‹çµæœåˆ†æï¼Œä½¿ç”¨è€…å¯èƒ½é—œå¿ƒçš„å•é¡Œï¼š
1. å¦‚ä½•é–‹å§‹ä½¿ç”¨ç›¸é—œå·¥å…·æˆ–æœå‹™ï¼Ÿ
2. è²»ç”¨å’Œæˆæœ¬è€ƒé‡æ˜¯ä»€éº¼ï¼Ÿ
3. æ•ˆæœå¤šä¹…èƒ½çœ‹åˆ°ï¼Ÿ
4. é©åˆå“ªäº›é¡å‹çš„ä¼æ¥­æˆ–å€‹äººï¼Ÿ
5. èˆ‡å…¶ä»–è§£æ±ºæ–¹æ¡ˆæœ‰ä½•å€åˆ¥ï¼Ÿ
"""
    
    if options.include_table:
        report += """
## ğŸ“‹ ç«¶çˆ­å°æ‰‹æ¯”è¼ƒè¡¨æ ¼

| æ’å | æ¨™é¡Œ | ç‰¹è‰² | ç›®æ¨™å—çœ¾ |
|------|------|------|----------|
"""
        for i, result in enumerate(serp_result.organic_results[:5], 1):
            title_short = result.title[:30] + "..." if len(result.title) > 30 else result.title
            report += f"| {i} | {title_short} | å¾…åˆ†æ | å¾…åˆ†æ |\n"
    
    report += f"""

---
*æ­¤å ±å‘Šç”± SEO Analyzer åŸºæ–¼çœŸå¯¦æœå°‹è³‡æ–™è‡ªå‹•ç”Ÿæˆ*  
*åˆ†æè³‡æ–™ä¾†æº: Google æœå°‹çµæœ (å…± {total_results} ç­†)*  
*ç”Ÿæˆæ™‚é–“: {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S')} UTC*"""
    
    return report


def create_error_response(error_code: str, message: str, details: Optional[dict] = None) -> ErrorResponse:
    """å»ºç«‹çµ±ä¸€æ ¼å¼çš„éŒ¯èª¤å›æ‡‰ã€‚

    Args:
        error_code: éŒ¯èª¤ä»£ç¢¼
        message: éŒ¯èª¤è¨Šæ¯
        details: éŒ¯èª¤è©³ç´°è³‡è¨Š

    Returns:
        ErrorResponse: æ¨™æº–æ ¼å¼çš„éŒ¯èª¤å›æ‡‰
    """
    error_detail = None
    if details:
        error_detail = ErrorDetail(
            field=details.get("field"),
            provided_length=details.get("provided_length"),
            max_length=details.get("max_length"),
            min_length=details.get("min_length"),
            provided_value=details.get("provided_value"),
            expected_format=details.get("expected_format"),
            job_id=details.get("job_id"),
            processing_time=details.get("processing_time"),
            retry_after=details.get("retry_after")
        )

    error_info = ErrorInfo(
        code=error_code,
        message=message,
        details=error_detail,
        timestamp=datetime.now(timezone.utc).isoformat()
    )

    return ErrorResponse(
        status="error",
        error=error_info.model_dump()
    )
